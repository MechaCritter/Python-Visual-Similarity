{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bcabbb7590d770",
   "metadata": {},
   "source": [
    "# Oxford5k VLAD anf Fisher Vector Retrieval Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the Oxford Flower dataset.\n",
    "2. Extract deep convolutional features (last conv layer) from the `vgg16` model.\n",
    "3. Train a VLAD model on these deep features.\n",
    "4. Perform image retrieval queries.\n",
    "5. Show the effect of PCA (reducing features by half before VLAD) on retrieval performance.\n",
    "6. An analogous procedure is made for Fisher Vectors\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Relja ArandjeloviÄ‡ and Andrew Zisserman, 'All About VLAD', Department of Engineering Science, University of Oxford. \\\n",
    "[2] Liangliang Wang and Deepu Rajan, \"An Image Similarity Descriptor for Classification Tasks,\" J. Vis. Commun.\n",
    "Image R., vol. 71, pp. 102847, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5e6d1733eebf3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "169b9b052dc0e9b4",
   "metadata": {},
   "source": [
    "from itertools import islice\n",
    "\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "# Our library imports\n",
    "from pyvisim.features import DeepConvFeature\n",
    "from pyvisim.encoders.vlad import VLADEncoder\n",
    "from pyvisim.encoders.fisher_vector import FisherVectorEncoder\n",
    "from pyvisim.datasets import OxfordFlowerDataset  # We'll create this in a moment\n",
    "from pyvisim._config import ROOT\n",
    "from pyvisim._utils import *\n",
    "from pyvisim.eval import retrieve_top_k_similar"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df569f572035be0c",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "**Note**: training k-means models takes quite a bit of time. In this notebook, a single `n_clusters = 256` will be used. Add more values to `NUM_CLUSTERS` to experiment with different cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "id": "ef9a676a290a13c6",
   "metadata": {},
   "source": [
    "NUM_CLUSTERS = 256\n",
    "IMAGE_SIZE = (224, 224)\n",
    "DIM_REDUCTION_FACTOR = 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0add8d2ea6e3f52",
   "metadata": {},
   "source": [
    "## 2. Declare the Oxford Flower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "8bc666f136bd5e5a",
   "metadata": {},
   "source": [
    "train_dataset = OxfordFlowerDataset(purpose='train')\n",
    "val_dataset = OxfordFlowerDataset(purpose='validation')\n",
    "print(\"Number of images in the dataset:\", len(train_dataset))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1b5d70eaedec419",
   "metadata": {},
   "source": [
    "### Plot some images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6d3bef8b291a2a",
   "metadata": {},
   "source": [
    "for i in range(5):\n",
    "    img, label, _ = train_dataset[i]\n",
    "    print(\"Image size:\", img.shape)\n",
    "    plot_image(img, title = f\"Label: {label}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8465663d29f27ba8",
   "metadata": {},
   "source": [
    "### 3. Extract deep convolutional features\n",
    "\n",
    "In the original paper <ref>[1]</ref>, `SIFT` and `RootSIFT` features were used. Hence, the default parameter of the encoding would be `RootSIFT`. However, here, I would like to demonstrate the usage of deep convolutional features, as mentioned in <ref>[2]</ref>.\n",
    "\n",
    "We use `DeepConvFeatureExtractor` from our code. For demonstration, we'll pick `vgg16` and the last conv layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "af5d4c798fa02c7d",
   "metadata": {},
   "source": [
    "extractor = DeepConvFeature(\n",
    "    model=vgg16(weights=VGG16_Weights.DEFAULT),\n",
    "    layer_index=-1,  # Last conv layer\n",
    "    spatial_encoding=True,)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a82fc50861718f39",
   "metadata": {},
   "source": [
    "### Declare the VLAD encoder"
   ]
  },
  {
   "cell_type": "code",
   "id": "934c964a7fab03ea",
   "metadata": {},
   "source": [
    "vlad_encoder_no_pca = VLADEncoder(feature_extractor=extractor)\n",
    "print(vlad_encoder_no_pca)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6d29f4a19dfe5d8",
   "metadata": {},
   "source": [
    "### Learn visual vocabulary from the images"
   ]
  },
  {
   "cell_type": "code",
   "id": "c13441c65b6b86ca",
   "metadata": {},
   "source": [
    "vlad_encoder_no_pca.learn(train_dataset, n_clusters=256)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fcaed5d404fec5d",
   "metadata": {},
   "source": [
    "If you have issue with the runtime (`KMeans` is infamous for having high time complexity), you can follow this procedure instead:\n",
    "1) Train the k-means and PCA models in advance\n",
    "2) Save the models using `joblib` or `pickle`. \n",
    "3) Pass the model as an argument to the `VLAD` class.\n",
    "\n",
    "The whole process is shown below:\n",
    "\n",
    "### 1.  Extract features\n",
    "\n",
    "**WARNING**: This loads all images and feature maps into memory. Make sure your computer can handle the data.\n",
    "\n",
    "```python\n",
    "NUM_IMGS = None # Set a value lower than `len(dataset)` if you run into memory issues\n",
    "labels, paths, features = [], [], []\n",
    "for img, lbl, path in islice(train_dataset, NUM_IMGS if NUM_IMGS is not None else len(train_dataset)):\n",
    "    labels.append(lbl)\n",
    "    paths.append(path)\n",
    "    features.append(extractor(img))\n",
    "\n",
    "labels = np.array(labels)\n",
    "features = np.vstack(features)\n",
    "```\n",
    "\n",
    "### 2. Train k-means and PCA models\n",
    "\n",
    "Note that **the `PCA` model has to be trained first**, since it reduces the dimensionality of the features before k-means clustering (you will get dimension mismatch if you did it the other way around).\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=features.shape[1] // 2, random_state=42)\n",
    "features = pca.fit_transform(features)\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(features)\n",
    "```\n",
    "\n",
    "### 3. Save the models\n",
    "\n",
    "```python\n",
    "def save_model(model, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "\n",
    "    :param model: Model to save\n",
    "    :param file_path: Path to save the trained model\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        joblib.dump(model, file)\n",
    "        \n",
    "save_model(pca, 'pca.pkl')\n",
    "save_model(kmeans, 'kmeans.pkl')\n",
    "```\n",
    "\n",
    "### 4. Load the models\n",
    "\n",
    "```python\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"\n",
    "    Load the trained model from a file.\n",
    "\n",
    "    :param file_path: Path to load the trained model\n",
    "    :return: Loaded model\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return joblib.load(file)\n",
    "\n",
    "pca = load_model('pca.pkl')\n",
    "kmeans = load_model('kmeans.pkl')\n",
    "```\n",
    "\n",
    "### 5. Pass the models to the VLAD encoder\n",
    "\n",
    "```python\n",
    "encoder = VLADEncoder(kmeans_model=kmeans, pca=pca)\n",
    "```\n",
    "After this step, you can follow the rest of this notebook like normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e138a27240f6c67",
   "metadata": {},
   "source": [
    "### Generate encoding map\n",
    "\n",
    "This generates a map `{image1_full_path: vector1, image2_full_path: vector2...}`. This method will come handy as we do image retrieval later on. for that, we first need to compute some variables from the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_IMGS = None # Set a value lower than `len(dataset)` if you run into memory issues. But be aware that without seeing the whole dataset, the retrieval performance will degrade,\n",
    "labels, paths, features = [], [], []\n",
    "for img, lbl, path in islice(train_dataset, NUM_IMGS if NUM_IMGS is not None else len(train_dataset)):\n",
    "    labels.append(lbl)\n",
    "    paths.append(path)\n",
    "    features.append(extractor(img))\n",
    "\n",
    "labels = np.array(labels)\n",
    "features = np.vstack(features)"
   ],
   "id": "df024255034f1001"
  },
  {
   "cell_type": "code",
   "id": "a9050c20fdfaa7fc",
   "metadata": {},
   "source": [
    "vlad_data = vlad_encoder_no_pca.generate_encoding_map(paths)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ce8a1dce8152a8e",
   "metadata": {},
   "source": [
    "Similar to above, but here, the dimension of each feature vector is reduced `by half` using `PCA`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336e1bc0fd1c7ec",
   "metadata": {},
   "source": [
    "### Declare the VLAD encoder (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a37cf4eab0941d21",
   "metadata": {},
   "source": [
    "vlad_encoder_with_pca = VLADEncoder(\n",
    "    feature_extractor=extractor,\n",
    ")\n",
    "vlad_encoder_with_pca.learn(train_dataset, n_clusters=NUM_CLUSTERS, dim_reduction_factor=2)\n",
    "vlad_data_pca = vlad_encoder_with_pca.generate_encoding_map(paths)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32a7c59ec470f6a0",
   "metadata": {},
   "source": [
    "## **5. Compare some images**\n",
    "\n",
    "We will now use the trained VLAD encoders to compute similarity between some images."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0869bc2dfce3437",
   "metadata": {},
   "source": [
    "image_1, label_1, path_1 = train_dataset[2005]\n",
    "image_2, label_2, path_2 =  val_dataset[401]\n",
    "plot_image(image_1)\n",
    "plot_image(image_2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c07e5482792e87e",
   "metadata": {},
   "source": [
    "Now, we compare the two images. `cosine similarity` is used in this case."
   ]
  },
  {
   "cell_type": "code",
   "id": "73232af235817368",
   "metadata": {},
   "source": [
    "sim_with_pca = vlad_encoder_with_pca.similarity_score(image_1, image_2)\n",
    "print(\"Similarity Score, with PCA: \", sim_with_pca)\n",
    "sim_without_pca = vlad_encoder_no_pca.similarity_score(image_1, image_2)\n",
    "print(\"Similarity Score, without PCA: \", sim_without_pca)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63a835e73d14452a",
   "metadata": {},
   "source": [
    "## **6. Fetch the most similar image in the dataset, given a query image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b397e0507db1108",
   "metadata": {},
   "source": [
    "Now, we will select an image in the validation dataset, on which the model is not yet trained:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd4237c2992924b6",
   "metadata": {},
   "source": [
    "query_image, query_label, query_path =  val_dataset[103]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c250e4ffc079fefc",
   "metadata": {},
   "source": [
    "Retrieve top-k most similar images using the datasets computed. We will see how it works, with and without PCA."
   ]
  },
  {
   "cell_type": "code",
   "id": "76cecc3b34d91f42",
   "metadata": {},
   "source": [
    "top_k_vlad_pca = retrieve_top_k_similar(query_image, vlad_data_pca, vlad_encoder_with_pca)\n",
    "for path, score in top_k_vlad_pca:\n",
    "    print(f\"Path: {os.path.basename(path)}, Cosine similarity: {score:.4f}\")\n",
    "\n",
    "tok_k_vlad_no_pca = retrieve_top_k_similar(query_image, vlad_data, vlad_encoder_no_pca)\n",
    "for path, score in tok_k_vlad_no_pca:\n",
    "    print(f\"\\nPath: {os.path.basename(path)}, Cosine similarity: {score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c13ad935ca5c08c",
   "metadata": {},
   "source": [
    "### Let's plot both images next to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f2a32591afa44",
   "metadata": {},
   "source": [
    "\n",
    "a) Using Model trained on data with PCA"
   ]
  },
  {
   "cell_type": "code",
   "id": "c34d93f22a771bdc",
   "metadata": {},
   "source": [
    "plt.imshow(query_image)\n",
    "plt.title(f\"Query image. Label: {query_label}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(top_k_vlad_pca)):\n",
    "    path, _ = top_k_vlad_pca[i]\n",
    "    label = train_dataset.labels[train_dataset.image_paths.index(path)]\n",
    "    plt.subplot(1, len(top_k_vlad_pca), i + 1)\n",
    "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Retrieved image. Label: {label}\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2369f859159edc7e",
   "metadata": {},
   "source": [
    "b) Using model trained on full data"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2892ae33554a7d3",
   "metadata": {},
   "source": [
    "plt.imshow(query_image)\n",
    "plt.title(f\"Query image. Label: {query_label}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(tok_k_vlad_no_pca)):\n",
    "    path, _ = tok_k_vlad_no_pca[i]\n",
    "    label = train_dataset.labels[train_dataset.image_paths.index(path)]\n",
    "    plt.subplot(1, len(tok_k_vlad_no_pca), i + 1)\n",
    "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Retrieved image. Label: {label}\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87f3f0d2ee77ea91",
   "metadata": {},
   "source": [
    "## **7. Similar to above, we will do the exact things for the Fisher Vector**\n",
    "\n",
    "The implementation for both VLAD and Fisher Vectors are identical. After all, VLAD is simply a simplified case of Fisher Vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6005a492360d12",
   "metadata": {},
   "source": "### Instantiate Fisher Vector Encoder"
  },
  {
   "cell_type": "code",
   "id": "c8da739a419c04ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:52:23.667668Z",
     "start_time": "2025-01-17T20:52:23.484962Z"
    }
   },
   "source": [
    "fisher_encoder_no_pca = FisherVectorEncoder(feature_extractor=extractor,)\n",
    "print(vlad_encoder_no_pca)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FisherVectorEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m fisher_encoder_no_pca \u001B[38;5;241m=\u001B[39m \u001B[43mFisherVectorEncoder\u001B[49m(feature_extractor\u001B[38;5;241m=\u001B[39mextractor,)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(vlad_encoder_no_pca)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'FisherVectorEncoder' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Learn visual vocabulary\n",
    "\n",
    "Fisher Vectors use `Gaussian Mixture Models` to learn visual vocabulary, which takes even longer to train than `KMeans`."
   ],
   "id": "82e809cc3ebcb66d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fisher_encoder_no_pca.learn(train_dataset)",
   "id": "9d86ffdbe27b3111"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate encoding map",
   "id": "a3d2714bb18cd332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fisher_data = fisher_encoder_no_pca.generate_encoding_map(paths)",
   "id": "f6f5431815920476"
  },
  {
   "cell_type": "markdown",
   "id": "8a9c9e5e93f477e9",
   "metadata": {},
   "source": [
    "Similar to above, if you run into memory issues, consider pre-training the model, loading them and then pass them as arguments. Simply replace the `KMeans` object with the Gaussian Mixture object, which can be imported as:\n",
    "\n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "```\n",
    "\n",
    "Otherwise, the implementation is identical to that of `VLADEncoder`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed83ef02328b932",
   "metadata": {},
   "source": [
    "### Instantiate Fisher Vector Encoder with PCA"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1d9f9c5f7b0d7aa",
   "metadata": {},
   "source": [
    "fisher_encoder_with_pca = FisherVectorEncoder(\n",
    "    feature_extractor=extractor\n",
    ")\n",
    "fisher_encoder_with_pca.learn(train_dataset, n_clusters=NUM_CLUSTERS, dim_reduction_factor=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fisher_data_pca = fisher_encoder_with_pca.generate_encoding_map(paths)",
   "id": "c0d6b2aaf56e95eb"
  },
  {
   "cell_type": "markdown",
   "id": "4afb94829b10f898",
   "metadata": {},
   "source": [
    "### Compute similarity of two images"
   ]
  },
  {
   "cell_type": "code",
   "id": "cfff9c3b96ada940",
   "metadata": {},
   "source": [
    "image_similarity_with_pca = fisher_encoder_with_pca.similarity_score(image_1, image_2)\n",
    "image_similarity_without_pca = fisher_encoder_no_pca.similarity_score(image_1, image_2)\n",
    "print(\"Fisher Similarity Score, with PCA: \", image_similarity_with_pca)\n",
    "print(\"Fisher Similarity Score, without PCA: \", image_similarity_without_pca)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "606d35e2622a7168",
   "metadata": {},
   "source": [
    "### Retrieve top-k most similar images"
   ]
  },
  {
   "cell_type": "code",
   "id": "45ad5bdd03538dd3",
   "metadata": {},
   "source": [
    "top_k_fisher_pca = retrieve_top_k_similar(query_image, fisher_data_pca, fisher_encoder_with_pca)\n",
    "print(\"Evaluation of fisher vector with PCA:\")\n",
    "for path, score in top_k_fisher_pca:\n",
    "    print(f\"Path: {os.path.basename(path)}, Cosine similarity: {score:.4f}\")\n",
    "\n",
    "top_k_fisher_no_pca = retrieve_top_k_similar(query_image, fisher_data, fisher_encoder_no_pca)\n",
    "print(\"\\nEvaluation of Fisher Vector without PCA:\")\n",
    "for path, score in top_k_fisher_no_pca:\n",
    "    print(f\"Path: {os.path.basename(path)}, Cosine similarity: {score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40c873bae24ce1a9",
   "metadata": {},
   "source": [
    "### Let's plot both images next to each other.\n",
    "a) Using Model trained on Data with PCA"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e3aca3dbbedb854",
   "metadata": {},
   "source": [
    "plt.imshow(query_image)\n",
    "plt.title(f\"Query image. Label: {query_label}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(top_k_fisher_pca)):\n",
    "    path, _ = top_k_fisher_pca[i]\n",
    "    label = train_dataset.labels[train_dataset.image_paths.index(path)]\n",
    "    plt.subplot(1, len(tok_k_vlad_no_pca), i + 1)\n",
    "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Retrieved image. Label: {label}\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc5fd2aa588c610f",
   "metadata": {},
   "source": [
    "b) Using Model trained on full data"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd80586a9a8312f8",
   "metadata": {},
   "source": [
    "plt.imshow(query_image)\n",
    "plt.title(f\"Query image. Label: {query_label}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(top_k_fisher_no_pca)):\n",
    "    path, _ = top_k_fisher_no_pca[i]\n",
    "    label = train_dataset.labels[train_dataset.image_paths.index(path)]\n",
    "    plt.subplot(1, len(top_k_fisher_no_pca), i + 1)\n",
    "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Retrieved image. Label: {label}\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15f219466d981b0f",
   "metadata": {},
   "source": [
    "## **7. Conclusion**\n",
    "\n",
    "We have demonstrated:\n",
    "1. Training VLAD and Fisher Vector Encoders on deep embeddings of VGG-16 Models extracted from images of the `Oxford 102 Flower Dataset`.\n",
    "2. Comparing PCA vs. No PCA approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
