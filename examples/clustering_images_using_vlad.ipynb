{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Oxford Flowers Clustering Notebook**\n",
    "\n",
    "In this notebook, we:\n",
    "1. Load the validation and test splits of the Oxford Flower Dataset, merging them.\n",
    "2. Compute (or load) the VLAD vectors from the deep extractor.\n",
    "3. Cluster these images into 102 clusters (the number of classes).\n",
    "4. Compute ARI, NMI, and interpret the results.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setup and Load Data**\n"
   ],
   "id": "caa42c037a2d3d9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:51:47.480251Z",
     "start_time": "2025-01-05T12:51:38.039773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.datasets import OxfordFlowerDataset\n",
    "from src.utils import cluster_images_and_generate_statistics\n",
    "from src.utils import *\n",
    "from src.features._features import DeepConvFeatureExtractor\n",
    "from src.metrics.vlad import VLADEncoder\n",
    "from src.config import ROOT, DEVICE"
   ],
   "id": "9d64d8aebd89bdcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we are using the deep extractor, we need to resize the images to 224x224 pixels and turn them into tensors.",
   "id": "ec4524b81c769cd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:51:47.498280Z",
     "start_time": "2025-01-05T12:51:47.493273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))\n",
    "])"
   ],
   "id": "bda32d56f24ed36d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the validation and test datasets.",
   "id": "3cb6bead25fe31f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:51:47.870278Z",
     "start_time": "2025-01-05T12:51:47.743009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_dataset = OxfordFlowerDataset(\n",
    "    purpose=\"validation\",\n",
    "    transform=transformer\n",
    ")\n",
    "\n",
    "test_dataset = OxfordFlowerDataset(\n",
    "    purpose=\"test\",\n",
    "    transform=transformer\n",
    ")"
   ],
   "id": "a14cfee4a8bef712",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge them",
   "id": "698262ff49c761a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:52:00.265353Z",
     "start_time": "2025-01-05T12:51:47.903770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images, labels = [], []\n",
    "for (val_img, val_label, _), (test_img, test_label, _) in zip(val_dataset, test_dataset):\n",
    "    images.append(val_img)\n",
    "    images.append(test_img)\n",
    "    labels.append(val_label)\n",
    "    labels.append(test_label)"
   ],
   "id": "a5b385c6b803ab7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2. Compute VLAD vectors**\n",
   "id": "19c2a745e8b7c354"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.1 Load the PCA and KMeans models**\n",
    "\n",
    "This requires that you already trained your PCA and KMeans models on the deep features extracted from the Oxford Flowers dataset and saved them as `pickle` files. If not, you can go to notebook `vlad_with_vgg16_embeddings.ipynb` to train them."
   ],
   "id": "7a13dab62c5e1ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:52:00.294660Z",
     "start_time": "2025-01-05T12:52:00.280279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pca = load_model(rf'{ROOT}/models/pickle_model_files/pca_vlad_k256_deep_features_vgg16_feature_dim257.pkl')\n",
    "k_means = load_model(rf'{ROOT}/models/pickle_model_files/k_means_k256_deep_features_vgg16_pca.pkl')"
   ],
   "id": "aff8ecfac713dc5f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.2 Define the feature extractor and the VLAD encoder**",
   "id": "56ecdfd333a2643d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:52:01.722803Z",
     "start_time": "2025-01-05T12:52:00.330036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extractor = DeepConvFeatureExtractor(\n",
    "    model=vgg16(weights=VGG16_Weights.DEFAULT),\n",
    "    layer_index=-1,  # Last conv layer\n",
    "    append_spatial_coords=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "vlad_encoder = VLADEncoder(\n",
    "    feature_extractor=extractor,\n",
    "    kmeans_model=k_means,\n",
    "    pca=pca,\n",
    "    power_norm_weight=1.0,\n",
    ")"
   ],
   "id": "1ec2dcdea25d9e45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-05 13:52:01,717 - Feature_Extractor - INFO - Device used: cuda\n",
      "2025-01-05 13:52:01,719 - Feature_Extractor - INFO - Selected layer: features.28, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5ea60b428ee463"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.3 Compute the VLAD vectors for both the validation and test splits**",
   "id": "d1a56c1a5d5d25c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:52:17.950837Z",
     "start_time": "2025-01-05T12:52:01.755283Z"
    }
   },
   "cell_type": "code",
   "source": "vlad_vectors = vlad_encoder.transform((img for img in images))",
   "id": "1f4f59996efba688",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3. Cluster into 102 Clusters and Compute ARI, NMI**\n",
    "\n",
    "`102` is the number of classes in the Oxford Flowers dataset. We want to see how well the clustering algorithm can cluster the images into these classes.\n"
   ],
   "id": "b3e79a4a4fe5134a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T12:53:36.281682Z",
     "start_time": "2025-01-05T12:52:17.983670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = 102\n",
    "results = cluster_images_and_generate_statistics(\n",
    "    features=vlad_vectors,        # The subset corresponding to val+test\n",
    "    true_labels=np.array(labels), # The true labels\n",
    "    n_clusters=num_classes,\n",
    "    method='kmeans'\n",
    ")\n",
    "\n",
    "print(f\"Clustering with KMeans into {num_classes} clusters:\")\n",
    "print(\"RI:\", results[\"ri\"])\n",
    "print(\"ARI:\", results[\"ari\"])\n",
    "print(\"NMI:\", results[\"nmi\"])\n"
   ],
   "id": "acb5afadfcd26901",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUD_VuNhat\\AppData\\Local\\anaconda3\\envs\\conda_env\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with KMeans into 102 clusters:\n",
      "RI: 0.7258474454028792\n",
      "ARI: 0.009982034303317965\n",
      "NMI: 0.1241525345925399\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6. Conclusion**\n",
    "\n",
    "We've demonstrated:\n",
    "- How to handle the mismatch between ground-truth class IDs (1..102) and cluster labels (0..101).\n",
    "- How to cluster images directly on deep-based VLAD vectors.\n",
    "- How to compute ARI and NMI for objective evaluation.\n"
   ],
   "id": "76c5e6adcfdc29d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
