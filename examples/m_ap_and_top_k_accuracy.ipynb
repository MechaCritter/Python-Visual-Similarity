{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Computing `Mean Average Precision (mAP)` and `Top-k Accuracy` for our Retrieval System**\n",
    "We'll sample 1000 random images in the `validation dataset` as queries. For each query:\n",
    "1. Retrieve all images, rank them by similarity.\n",
    "2. Compute average precision for each query.\n",
    "3. Take the mean across all queries => mAP. This takes into consideration the ranking of the images.\n",
    "4. We will evaluate top 1 accuracy and top-k accuracy."
   ],
   "id": "bf8480748f688a15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **1. Import Necessary Libraries**",
   "id": "4a6d104abe78a29b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:45.069222Z",
     "start_time": "2025-01-06T11:24:38.637544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import islice\n",
    "from torchvision import transforms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "from scripts.evaluate import retrieve_top_k_similar, top_k_accuracy, compute_map_top_k\n",
    "from src.datasets import OxfordFlowerDataset\n",
    "from src.features import DeepConvFeature\n",
    "from src.encoders import VLADEncoder, FisherVectorEncoder\n",
    "from src.utils import cosine_similarity, load_model\n",
    "from src.config import ROOT, DEVICE"
   ],
   "id": "883c6b24ba9b2d07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vunha\\anaconda3\\envs\\conda_env\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Declare Datasets",
   "id": "ab924ba0ac7eabff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:45.154045Z",
     "start_time": "2025-01-06T11:24:45.079733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = OxfordFlowerDataset(purpose=\"train\")\n",
    "val_dataset = OxfordFlowerDataset(purpose=\"validation\")"
   ],
   "id": "2f45338f1278f73",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3. Deep Conv Feature Extractor**\n",
   "id": "ada7c8083a999ccc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:45.929558Z",
     "start_time": "2025-01-06T11:24:45.341355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extractor = DeepConvFeature(\n",
    "    model=vgg16(weights=VGG16_Weights.DEFAULT),\n",
    "    layer_index=-1,  # Last conv layer\n",
    "    spatial_encoding=True,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "id": "eb8db9816c5f4687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 12:24:45,927 - Feature_Extractor - INFO - Selected layer: features.28, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2. Load the PCA and KMeans models**",
   "id": "e6aed9260089fa85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:45.948741Z",
     "start_time": "2025-01-06T11:24:45.936656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kmeans_model = load_model(rf'{ROOT}/models/pickle_model_files/k_means_k256_deep_features_vgg16_no_pca.pkl')\n",
    "kmeans_model_pca = load_model(rf'{ROOT}/models/pickle_model_files/k_means_k256_deep_features_vgg16_pca.pkl')\n",
    "pca_model_vlad = load_model(rf'{ROOT}/models/pickle_model_files/pca_vlad_k256_deep_features_vgg16_feature_dim257.pkl')"
   ],
   "id": "3a196893e7052962",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you have not yet trained your model or saved it, you can train it using the following code (might take a while. Reduce num_clusters to make it faster. This comes at the cost of the performance, however).",
   "id": "7152fbf688cea278"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:49.745298Z",
     "start_time": "2025-01-06T11:24:45.958256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CLUSTERS = 256\n",
    "IMAGE_SIZE = (224, 224)\n",
    "DIM_REDUCTION_FACTOR = 2\n",
    "\n",
    "labels, paths, features = [], [], []\n",
    "for img, lbl, path in train_dataset:\n",
    "    labels.append(lbl)\n",
    "    paths.append(path)\n",
    "    features.append(extractor(img))\n",
    "\n",
    "labels = np.array(labels)\n",
    "features = np.vstack(features)\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans_model.fit(features)\n",
    "\n",
    "pca_model_vlad = PCA(n_components=NUM_CLUSTERS // DIM_REDUCTION_FACTOR)\n",
    "pca_model_vlad.fit(features)\n",
    "reduced_features = pca_model_vlad.transform(features)\n",
    "\n",
    "kmeans_model_pca = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans_model_pca.fit(reduced_features)"
   ],
   "id": "4942ac6cb8495387",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m NUM_IMGS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n\u001B[0;32m      6\u001B[0m labels, paths, features \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlbl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mislice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_IMGS\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlbl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpaths\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni_Infos\\Bachelorarbeit\\workspace\\similarity_metrics_of_images\\src\\datasets.py:431\u001B[0m, in \u001B[0;36mOxfordFlowerDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    428\u001B[0m img_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[idx]\n\u001B[0;32m    429\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 431\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[0;32m    434\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(image)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3. Load the Encoders**",
   "id": "827aae8c87ff00fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:51.218412Z",
     "start_time": "2025-01-06T11:24:51.214408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vlad_encoder = VLADEncoder(\n",
    "    feature_extractor=extractor,\n",
    "    kmeans_model=kmeans_model_pca,\n",
    "    pca=pca_model_vlad,\n",
    "    power_norm_weight=1.0,\n",
    ")\n",
    "\n",
    "# TODO: declare other encoders also and loop"
   ],
   "id": "9c272e37f6765498",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **4. Sample random images as queries**",
   "id": "2f567572fc9d74c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:24:54.011749Z",
     "start_time": "2025-01-06T11:24:53.782423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_size = 1000\n",
    "idxs = random.sample(range(len(val_dataset)), sample_size)\n",
    "\n",
    "queries = []\n",
    "query_labels = []\n",
    "for idx in idxs:\n",
    "    queries.append(val_dataset[idx][0])\n",
    "    query_labels.append(val_dataset[idx][1])"
   ],
   "id": "bc60bba81ebf9d4e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **5. Compute the mAP**",
   "id": "f4dce5e120d66eda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First,we prepare the data.",
   "id": "4b383b42b69da8dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:25:03.476274Z",
     "start_time": "2025-01-06T11:24:56.296133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_paths, train_labels = zip(*[(path, label) for _, label, path in train_dataset])\n",
    "train_dataset_vectors = vlad_encoder.generate_encoding_map(train_paths)\n",
    "dataset_labels_dict = dict(zip(train_paths, train_labels))"
   ],
   "id": "d25449aac047c1d0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How it works:\n",
    "- If `k` is given, we only consider the `top-k` ranked results per query.\n",
    "- if `k=None` or omitted, we consider all results (the entire dataset).\n",
    "- For each query, we compute average precision (AP). Then we average across all queries, yielding mean average precision (mAP).\n",
    "\n",
    "First, we do it for the whole dataset:"
   ],
   "id": "1bf9a67fbbef7c11"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T11:38:54.486165Z",
     "start_time": "2025-01-06T11:31:37.730435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mAP_value = compute_map_top_k(\n",
    "    queries=queries,\n",
    "    query_labels=query_labels,\n",
    "    dataset=train_dataset_vectors,  # {path: vector}\n",
    "    dataset_labels=dataset_labels_dict,    # {path: label}\n",
    "    encoder=vlad_encoder  # or vlad_encoder, fisher_encoder\n",
    ")\n",
    "\n",
    "print(\"Mean Average Precision (mAP):\", mAP_value)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mAP_value \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_map\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# {path: vector}\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_labels_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# {path: label}\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvlad_encoder\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# or vlad_encoder, fisher_encoder\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Average Precision (mAP):\u001B[39m\u001B[38;5;124m\"\u001B[39m, mAP_value)\n",
      "File \u001B[1;32m~\\Documents\\Uni_Infos\\Bachelorarbeit\\workspace\\similarity_metrics_of_images\\scripts\\evaluate.py:515\u001B[0m, in \u001B[0;36mcompute_map\u001B[1;34m(queries, query_labels, dataset, dataset_labels, encoder)\u001B[0m\n\u001B[0;32m    513\u001B[0m relevant_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    514\u001B[0m precision_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m--> 515\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m rank, path \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sorted_paths, start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset_labels[path] \u001B[38;5;241m==\u001B[39m true_label:\n\u001B[0;32m    517\u001B[0m         relevant_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.2.3\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1220\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.2.3\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1235\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normally, we might only care about the top results. Let's compute the mAP for the top 5 results:",
   "id": "ff0af2df9a03ed12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mAP_value_top5 = compute_map_top_k(\n",
    "    queries=queries,\n",
    "    query_labels=query_labels,\n",
    "    dataset=train_dataset_vectors,\n",
    "    dataset_labels=dataset_labels_dict,\n",
    "    encoder=vlad_encoder,\n",
    "    k=5\n",
    ")\n",
    "print(\"Mean Average Precision (mAP) for Top-5:\", mAP_value_top5)"
   ],
   "id": "a1a45ca59f733984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6. Top-k accuracy**\n",
    "\n",
    "How it works:\n",
    "- For each query, retrieve **top-k** most similar images.\n",
    "- If any of them share the same label as the query, that counts as correct.\n",
    "- The final accuracy is `num_correct_queries / num_queries`.\n",
    "\n",
    "Let's compute the top-1 accuracy (the most relevant match has to be the correct one):"
   ],
   "id": "a64ee91ac15993f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:40:25.609290Z",
     "start_time": "2025-01-06T11:40:17.007566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc_k5 = top_k_accuracy(\n",
    "    queries=queries,\n",
    "    query_labels=query_labels,\n",
    "    dataset=train_dataset_vectors,\n",
    "    dataset_labels=dataset_labels_dict,\n",
    "    encoder=vlad_encoder,\n",
    "    k=1\n",
    ")\n",
    "print(\"Top-5 Accuracy:\", acc_k5)"
   ],
   "id": "50434ff67df04430",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy: 0.18\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normally, we might also consider the second, third and so on.. most relevant results. In this case, we can set `k > 1`. Let's try for `k=5`:",
   "id": "8aba5dd7bfdb19f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "acc_k5 = top_k_accuracy(\n",
    "    queries=queries,\n",
    "    query_labels=query_labels,\n",
    "    dataset=train_dataset_vectors,\n",
    "    dataset_labels=dataset_labels_dict,\n",
    "    encoder=vlad_encoder,\n",
    "    k=5\n",
    ")\n",
    "print(\"Top-5 Accuracy:\", acc_k5)"
   ],
   "id": "9b3f1f94448581ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
