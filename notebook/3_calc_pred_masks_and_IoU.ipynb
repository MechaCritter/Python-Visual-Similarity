{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "77f0efde90a6ad53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T11:16:37.711739Z",
     "start_time": "2024-12-08T11:16:37.695576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from segmentation_models_pytorch.utils.metrics import IoU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import save_to_hdf5, load_hdf5, load_model, multiclass_iou\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.config import IMAGE_SIZE, TRANSFORMER, DEVICE, ROOT\n",
    "from models.Segmentation import DeepLabV3Model, DeepLabV3PlusModel, PyramidAttentionNetworkModel, UNetModel"
   ],
   "id": "2f7ce4361f2319d5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Root",
   "id": "4e329e2ecdbe880b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T11:16:38.940552Z",
     "start_time": "2024-12-08T11:16:38.937028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = ROOT\n",
    "batch_size = 1"
   ],
   "id": "55703a431c275fda",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Initialize models\n",
    "\n",
    "**Note**: UNEt performs quite badly (only achieves `val IoU` of 0.78)."
   ],
   "id": "d4de6564b6b60cda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T11:16:41.350220Z",
     "start_time": "2024-12-08T11:16:40.334985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DeepLabV3\n",
    "dlv3 =DeepLabV3Model().model\n",
    "dlv3.load_state_dict(torch.load(f'{root}/models/torch_model_files/DeepLabV3_HybridFocalDiceLoss.pt'))\n",
    "dlv3.to(DEVICE)\n",
    "dlv3.eval()\n",
    "\n",
    "# DeepLabV3Plus\n",
    "dlv3p = DeepLabV3PlusModel().model\n",
    "dlv3p.load_state_dict(torch.load(f'{root}/models/torch_model_files/DeepLabV3Plus_HybridFocalDiceLoss.pt'))\n",
    "dlv3p.to(DEVICE)\n",
    "dlv3p.eval()\n",
    "\n",
    "# UNet\n",
    "unet = UNetModel().model\n",
    "unet.load_state_dict(torch.load(f'{root}/models/torch_model_files/UNet_HybridFocalDiceLoss.pt'))\n",
    "unet.to(DEVICE)\n",
    "unet.eval()\n",
    "\n",
    "# Pyramid Attention Network\n",
    "pan = PyramidAttentionNetworkModel().model\n",
    "pan.load_state_dict(torch.load(f'{root}/models/torch_model_files/PyramidAttentionNetwork_HybridFocalDiceLoss.pt'))\n",
    "pan.to(DEVICE)\n",
    "pan.eval()"
   ],
   "id": "1f2f78cb5a2d7112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:40,534 - DeepLabV3 - INFO - New <class 'segmentation_models_pytorch.decoders.deeplabv3.model.DeepLabV3'> model created with the following info:\n",
      "                            - Encoder name: resnet18\n",
      "                            - Activation: softmax\n",
      "                            - Classes: 12\n",
      "2024-12-08 12:16:40,534 - DeepLabV3 - INFO - Device used for model: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vunha\\AppData\\Local\\Temp\\ipykernel_42428\\2943846281.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dlv3.load_state_dict(torch.load(f'{root}/models/torch_model_files/DeepLabV3_HybridFocalDiceLoss.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:40,823 - DeepLabV3Plus - INFO - New <class 'segmentation_models_pytorch.decoders.deeplabv3.model.DeepLabV3Plus'> model created with the following info:\n",
      "                            - Encoder name: resnet18\n",
      "                            - Activation: softmax\n",
      "                            - Classes: 12\n",
      "2024-12-08 12:16:40,823 - DeepLabV3Plus - INFO - Device used for model: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vunha\\AppData\\Local\\Temp\\ipykernel_42428\\2943846281.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dlv3p.load_state_dict(torch.load(f'{root}/models/torch_model_files/DeepLabV3Plus_HybridFocalDiceLoss.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:41,055 - UNet - INFO - New <class 'segmentation_models_pytorch.decoders.unet.model.Unet'> model created with the following info:\n",
      "                            - Encoder name: resnet18\n",
      "                            - Activation: softmax\n",
      "                            - Classes: 12\n",
      "2024-12-08 12:16:41,055 - UNet - INFO - Device used for model: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vunha\\AppData\\Local\\Temp\\ipykernel_42428\\2943846281.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(f'{root}/models/torch_model_files/UNet_HybridFocalDiceLoss.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:41,274 - PyramidAttentionNetwork - INFO - New <class 'segmentation_models_pytorch.decoders.pan.model.PAN'> model created with the following info:\n",
      "                            - Encoder name: resnet18\n",
      "                            - Activation: softmax\n",
      "                            - Classes: 12\n",
      "2024-12-08 12:16:41,274 - PyramidAttentionNetwork - INFO - Device used for model: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vunha\\AppData\\Local\\Temp\\ipykernel_42428\\2943846281.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pan.load_state_dict(torch.load(f'{root}/models/torch_model_files/PyramidAttentionNetwork_HybridFocalDiceLoss.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PAN(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): PANDecoder(\n",
       "    (fpa): FPABlock(\n",
       "      (branch1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (mid): Sequential(\n",
       "        (0): ConvBnRelu(\n",
       "          (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (down1): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(512, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (down2): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (down3): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ConvBnRelu(\n",
       "          (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvBnRelu(\n",
       "        (conv): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv1): ConvBnRelu(\n",
       "        (conv): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (gau3): GAUBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (conv2): ConvBnRelu(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (gau2): GAUBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (conv2): ConvBnRelu(\n",
       "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (gau1): GAUBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): ConvBnRelu(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sigmoid()\n",
       "      )\n",
       "      (conv2): ConvBnRelu(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "92050708a12968bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T11:16:52.095764Z",
     "start_time": "2024-12-08T11:16:51.870446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train', transform=TRANSFORMER,one_hot_encode_mask=True)\n",
    "print(\"Number of training samples:\", num_train_imgs:=len(train_dataset))\n",
    "val_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='validation', transform=TRANSFORMER, one_hot_encode_mask=True)\n",
    "print(\"Number of test samples:\", num_val_imgs:=len(val_dataset))"
   ],
   "id": "179bd9fc0e4693c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1782\n",
      "Number of test samples: 187\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save predicted masks",
   "id": "1022b1e7e9891b8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T12:10:14.361336Z",
     "start_time": "2024-12-08T12:10:14.334033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_iou_dlv3 = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_dlv3p = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_unet = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_pan = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_paths = []\n",
    "\n",
    "val_iou_dlv3 = torch.zeros(num_val_imgs, dtype=torch.float32, device=DEVICE)\n",
    "val_iou_dlv3p = torch.zeros(num_val_imgs, dtype=torch.float32, device=DEVICE)\n",
    "val_iou_unet = torch.zeros(num_val_imgs, dtype=torch.float32, device=DEVICE)\n",
    "val_iou_pan = torch.zeros(num_val_imgs, dtype=torch.float32, device=DEVICE)\n",
    "val_paths = []"
   ],
   "id": "84234215ecff9ec5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:34:30.909475Z",
     "start_time": "2024-12-08T13:32:44.348601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compute predicted masks for training set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(train_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    train_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    train_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    train_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    train_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    train_paths.append(paths)\n",
    "\n",
    "train_paths = [path.replace('|', '/') for path in train_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_pan)})\n",
    "\n",
    "# Compute predicted masks for validation set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(val_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    val_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    val_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    val_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    val_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    val_paths.append(paths)\n",
    "\n",
    "val_paths = [path.replace('|', '/') for path in val_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(val_paths, val_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(val_paths, val_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(val_paths, val_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(val_paths, val_iou_pan)})\n"
   ],
   "id": "1f88383ddd878368",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1782it [01:35, 18.60it/s]\n",
      "187it [00:09, 18.98it/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute pairwise IoU differences between training and validation set",
   "id": "63e6a192e4aa385e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:35:42.719231Z",
     "start_time": "2024-12-08T13:35:42.533499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_hdf5(file_path: str) -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load data from an HDF5 file.\n",
    "\n",
    "    :param file_path: Path to the HDF5 fileuse\n",
    "\n",
    "    :return: Dictionary containing data from the HDF5 file\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        data = {key: val[()] for key, val in file.items()}\n",
    "    return data\n",
    "\n",
    "train_data= load_hdf5(f'{root}/res/model_performance/train_iou_dlv3.h5')\n",
    "\n",
    "\n",
    "val_data = load_hdf5(f'{root}/res/model_performance/val_iou_dlv3.h5')\n",
    "val_data"
   ],
   "id": "6c1d95223a246fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'002583_jpg.rf.d9a73a4c7d0f131abe4afd1cf6cb7643.jpg': 0.8646055,\n",
       " '002584_jpg.rf.df7de4720f9fe5ab3d1ab2c8beae10aa.jpg': 0.90118337,\n",
       " '002588_jpg.rf.c15adcfef31164e442dac07f4446f4c5.jpg': 0.87565553,\n",
       " '002595_jpg.rf.9f2c0ffbe46253b2ac3456eb24edf23a.jpg': 0.76098657,\n",
       " '002596_jpg.rf.923de0d457c9ad49c78fe0c936d1a7c3.jpg': 0.60054594,\n",
       " '002599_jpg.rf.01c95c8b0b6ccdd565e68c2bd538e895.jpg': 0.8110784,\n",
       " '002600_jpg.rf.e1ae5a6e3ebf4268d25207a95a9ab49b.jpg': 0.79282916,\n",
       " '002605_jpg.rf.b33cad48b2068888dab6857f1e069607.jpg': 0.4241023,\n",
       " '002619_jpg.rf.c443b7e27337b81202ace674159f0268.jpg': 0.6540473,\n",
       " '002625_jpg.rf.03479900b5e6c512ce312646551a18df.jpg': 0.54075384,\n",
       " '002625_jpg.rf.18b3b6bb6e3cffdc1d852e614a3fa65d.jpg': 0.5229757,\n",
       " '002625_jpg.rf.b9602442e461429fcbabe4eaa2f265a6.jpg': 0.51529276,\n",
       " '002629_jpg.rf.95f4c38c9eff1e94bdb97cbb9a5ab7fa.jpg': 0.7778227,\n",
       " '002630_jpg.rf.da3c3f72e863204de7ace115999e04d7.jpg': 0.8463952,\n",
       " '002631_jpg.rf.ee4f7ee0ee7061108a7b41d44d363e69.jpg': 0.6679689,\n",
       " '002639_jpg.rf.c460011f10deb7c06a4f33039f0e8b62.jpg': 0.65415645,\n",
       " '002640_jpg.rf.9d58a9efbe80757b217b79151dc2f991.jpg': 0.9587747,\n",
       " '002643_jpg.rf.7ce93fb5525133a9bfb37986761b779a.jpg': 0.7265699,\n",
       " '002648_jpg.rf.1d05467a0e36c0d98f42d7971c7446f7.jpg': 0.4750982,\n",
       " '002648_jpg.rf.c655e4642a63f2692ff08caa352f42ac.jpg': 0.6720471,\n",
       " '002648_jpg.rf.da551f03fddc352d2693e0f869a9cee2.jpg': 0.4290454,\n",
       " '002657_jpg.rf.cd7c36dc74e576d40f42759cce1822af.jpg': 0.46712765,\n",
       " '002658_jpg.rf.5e65c45b841ecb640c0bb667685c43bf.jpg': 0.6256908,\n",
       " '002667_jpg.rf.ae5dd2a26b3e94a6f822265f1da91c9a.jpg': 0.8633113,\n",
       " '002668_jpg.rf.3f9dcb2f78e91f4555eeeae736f44a7b.jpg': 0.85904706,\n",
       " '002672_jpg.rf.5324fdf90acb2d0d9f15e3cac2ce65d6.jpg': 0.8514868,\n",
       " '002673_jpg.rf.95b51a309f49b623c6222638ba32b6d5.jpg': 0.70025295,\n",
       " '002675_jpg.rf.8c5bd6ae6b96c046d8fb9b4d4f473d78.jpg': 0.68286604,\n",
       " '002680_jpg.rf.fccb3641299e78df283bb3b692b61f1c.jpg': 0.74035513,\n",
       " '002681_jpg.rf.3f082114aab675a06ef3fa1773276818.jpg': 0.6909646,\n",
       " '002684_jpg.rf.2eed148f6bad3460658dd1679e45f107.jpg': 0.7330176,\n",
       " '002685_jpg.rf.0e8ae37af36a9e99e01575daa45a3f94.jpg': 0.6184592,\n",
       " '002686_jpg.rf.8fc8251c9d62418b2eb7148ef2cd1b1f.jpg': 0.7961279,\n",
       " '002691_jpg.rf.10154670da23eedbd35fd735fa1c5507.jpg': 0.8554944,\n",
       " '002712_jpg.rf.8a4d4073820f9d519deb43cd86cd056d.jpg': 0.7799053,\n",
       " '002714_jpg.rf.aa2d6e1852eb9634e8122e1190514bb2.jpg': 0.8930631,\n",
       " '002717_jpg.rf.fc9eb11f7a8fcfc8e752b4d602ea9cf0.jpg': 0.94710714,\n",
       " '002719_jpg.rf.f9e398eb30f6cc05010c3d8e06c9e0e3.jpg': 0.89199615,\n",
       " '002725_jpg.rf.4ed0e733a0ca647918220874e32736f5.jpg': 0.8596546,\n",
       " '002732_jpg.rf.3f134e1acbd7797224c9c88ee3d5b33d.jpg': 0.59998614,\n",
       " '002743_jpg.rf.bdf56bc3cebba4597f4e8e5a0e1dd857.jpg': 0.8924828,\n",
       " '002745_jpg.rf.b8a5f4fd62311d5c80d119a16c687648.jpg': 0.77083325,\n",
       " '002750_jpg.rf.c69c65e5acc4887d5e73555dc588e756.jpg': 0.8489629,\n",
       " '002755_jpg.rf.ea6363a59356abfdc12044093f4c7139.jpg': 0.7394922,\n",
       " '002756_jpg.rf.9ed0dfa8a558f0945dae7ad040ca501a.jpg': 0.7293177,\n",
       " '002757_jpg.rf.c6ac3e70a248b2332d09d4e06bb04ade.jpg': 0.7185219,\n",
       " '002766_jpg.rf.94e4c04edf371bc6ea13bd9a0adee1d7.jpg': 0.8731623,\n",
       " '002768_jpg.rf.04ce564e19c406818af86218df0e547a.jpg': 0.5360585,\n",
       " '002768_jpg.rf.61e252237676f82b1199d2cab0f01bd1.jpg': 0.56669205,\n",
       " '002769_jpg.rf.92d80a592faeea9ab8380d67381a183d.jpg': 0.8507676,\n",
       " '002770_jpg.rf.74af3fcd86db63bc19a67939bb5f2551.jpg': 0.7654531,\n",
       " '002772_jpg.rf.e3f1505b51acb478d6787c2015398394.jpg': 0.5665603,\n",
       " '002774_jpg.rf.1a84d0bf5f11c607ee30715180be1af7.jpg': 0.78892195,\n",
       " '002775_jpg.rf.1d3b1fbfff5bde5813d6a7e240f3638d.jpg': 0.41952938,\n",
       " '002779_jpg.rf.7aa4813c31519a14a654146fa3e4e183.jpg': 0.60305935,\n",
       " '002788_jpg.rf.a6c97b2715beb71fa006c20f33008730.jpg': 0.8733169,\n",
       " '002792_jpg.rf.4451912b75e3248c5408c6bb4727125a.jpg': 0.6137006,\n",
       " '002792_jpg.rf.5fa1bc63435afc3b8b9e2cde44af5c96.jpg': 0.61993456,\n",
       " '002792_jpg.rf.608e74a9e6f5c8d14ac46d6628ad41e5.jpg': 0.42005455,\n",
       " '002807_jpg.rf.b483755ea74dffbcf23aa5134f2acf52.jpg': 0.7023899,\n",
       " '002808_jpg.rf.2dc87beb59e8334b284bbf479c219cd0.jpg': 0.78813946,\n",
       " '002810_jpg.rf.9a2f2dbcb84f8e7480dd353c97254d1f.jpg': 0.70774317,\n",
       " '002811_jpg.rf.4bd3b503b9464ced8642a90b4d8471b5.jpg': 0.6950012,\n",
       " '002815_jpg.rf.34a83bafb86269cb53ca927fa9e01c6f.jpg': 0.83916414,\n",
       " '002829_jpg.rf.619fa2e42c6154a1bd70a6ed4fa7dc53.jpg': 0.8063397,\n",
       " '002831_jpg.rf.6cd9cbeb3f3b4ee03e6afd1ad5638f64.jpg': 0.6585091,\n",
       " '002832_jpg.rf.e2a5f4bca0fc4e096c99d3f2adeddfa7.jpg': 0.57084346,\n",
       " '002841_jpg.rf.6f4825c54a3b2d3d0c26e1eecfef8272.jpg': 0.7940347,\n",
       " '002844_jpg.rf.c24f98bab28d1b8eb7e9a64257437b1d.jpg': 0.86775017,\n",
       " '0039502_jpg.rf.a1d10528079cae2de516c5aaf19367a1.jpg': 0.8344814,\n",
       " '0039808_jpg.rf.38ee88fd659ac2836efec631f0eba856.jpg': 0.9603273,\n",
       " '0039914_jpg.rf.17bfd223642712c48854607df6048e28.jpg': 0.717761,\n",
       " '0039993_jpg.rf.570c457ef1bb8ac725a986a637a1afee.jpg': 0.6030113,\n",
       " '0040094_jpg.rf.dce498bce484ef500e7e67f54fb0850d.jpg': 0.9625954,\n",
       " '0040323_jpg.rf.aa1e8c3656e18589ba58849380b08341.jpg': 0.9157617,\n",
       " '0040705_jpg.rf.ef92deff4289682bbf0cc32219c3ce4b.jpg': 0.9142367,\n",
       " '0040986_jpg.rf.b338bac8d710f9d7df14748102112883.jpg': 0.9322728,\n",
       " '0040996_jpg.rf.5ce52c4dabf08c1e89d207dc200fb120.jpg': 0.7877122,\n",
       " '0041022_jpg.rf.42498dac515cd9745c60fb9dca40cdff.jpg': 0.92690945,\n",
       " '0041043_jpg.rf.dec9f29fcfe347d16974c03dbb448353.jpg': 0.89497143,\n",
       " '0041081_jpg.rf.ad32097aa3f72ed3800622d99a2fdda9.jpg': 0.85787296,\n",
       " '0041235_jpg.rf.eba9d213961283f82825fe7927431ea4.jpg': 0.9084487,\n",
       " '0041367_jpg.rf.6edd4c088d52fa2e64eebcb896811803.jpg': 0.8436731,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpeg_jpg.rf.39a71454875804716f00015499ab9fdc.jpg': 0.9411547,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpg.rf.549810d71fd79988de490def6ec3d6be.jpg': 0.93736726,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpg.rf.9468599736a68eb0d1c257628866f08a.jpg': 0.5668842,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpg.rf.d7f2cc691eb85ac09c2f5a441738b6bd.jpg': 0.6520228,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpg.rf.dcdcdeb898bc5406bfb8da24f9289f74.jpg': 0.9564237,\n",
       " '07afc9ffb88b74734ca2963876605d21_jpg.rf.df2f97962fd253b689a0be3e16d14a2d.jpg': 0.9422936,\n",
       " '0e99831afc66dbdea48b081358b7c3f5_jpeg_jpg.rf.9c1bf66385a2100753cd54a11117caf8.jpg': 0.8903929,\n",
       " '0ed4301ae1ddf7d55b74c96d8d10ac63_jpeg_jpg.rf.0dfa9679d2c91a67ac93a47883645a13.jpg': 0.89055026,\n",
       " '1017_png_jpg.rf.9879439fd2e7377f1e2663fc282afb47.jpg': 0.9082595,\n",
       " '1027_png_jpg.rf.180529ad4bb8210854c8882969b24361.jpg': 0.9842123,\n",
       " '1575_png_jpg.rf.01155d9bc3e383447fc91c9de38c80e9.jpg': 0.9643228,\n",
       " '1576_png_jpg.rf.1d7069b61780589a4afa23ebf8f8ba22.jpg': 0.9818168,\n",
       " '1621_png_jpg.rf.d7fe9bf960710e493f4372e9ab7341cb.jpg': 0.91978353,\n",
       " '2253_png_jpg.rf.d281b07266b5edbbc018b5403268c380.jpg': 0.8233206,\n",
       " '2257_png_jpg.rf.079974ab3e9e40ed9a2e07ed78977cc6.jpg': 0.95456916,\n",
       " '2496_png_jpg.rf.eab03751f22ab66dafe5d53560781639.jpg': 0.97545666,\n",
       " '2712_png_jpg.rf.5cc3834cf85b3deb8ef8ae1b09b6c621.jpg': 0.9505655,\n",
       " '2778_png_jpg.rf.331d2fc86faba71702a5c575bcf529e9.jpg': 0.97293186,\n",
       " '2913_png_jpg.rf.122186d26c0736381010b430c0c4135c.jpg': 0.48816234,\n",
       " '3402_png_jpg.rf.e20f2c16b67cea33dc26ea681580f08d.jpg': 0.9837998,\n",
       " '3b73d0bf347e7e753cdc16fe543efbc5_jpg.rf.214dca4bb7b335ef3947af9318aaba74.jpg': 0.9101551,\n",
       " '42aff41e6b7f3947782eaac8e45ba941_jpg.rf.d4a6c11598feca71a03e99cf711fd21d.jpg': 0.90338844,\n",
       " '4845_png_jpg.rf.5c53aa73b971e7d2f20507ec52bd024c.jpg': 0.98754245,\n",
       " '4c1887d3c2f07948471f6ff31f8f75b2_jpeg_jpg.rf.e97252e7cff8d9a4f434760d7d010948.jpg': 0.7775575,\n",
       " '5425_png_jpg.rf.5e93957d3212d4392b7da4e48cd4f72e.jpg': 0.95216316,\n",
       " '5471_png_jpg.rf.ea2aff2ced16d82931b99c48be7ef527.jpg': 0.9487551,\n",
       " '5769a280601765b4622a0df939b5cd71_jpeg_jpg.rf.534059c3c39feaac9f0e3641ffaa3858.jpg': 0.92977357,\n",
       " '5c28568f878b851c5baaa3a9f4280501_jpeg_jpg.rf.9efb683a61ac4d7fc5772f09860eefa5.jpg': 0.7393788,\n",
       " '63aa050b18c7eb851aa631a8453835af_jpeg_jpg.rf.272312e44dcaa62b4546fae8fe73ac5c.jpg': 0.77780455,\n",
       " '6918d2cc3a6d92293239ad9a2b7f39b1_jpeg_jpg.rf.e63c4d9140491c4eb26c49519718d9d6.jpg': 0.89116126,\n",
       " '6f877db90907db244484b1d27d5de353_jpeg_jpg.rf.a29999cb096861b20ab6cfbfbc60733a.jpg': 0.93413556,\n",
       " '7969e5a1f3327d10938c2bc328bcb4b1_jpg.rf.d659d3dfb01d8771c405ec7dc197dad2.jpg': 0.86304873,\n",
       " '8a2d4d21d2f9d45370b36febc827a63f_jpeg_jpg.rf.94648df03fce70bb71f67d2c3bbf01ad.jpg': 0.7084707,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.1af38b770384ee65728cc9c3e9a54b15.jpg': 0.96797,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.711beca5899747e254c4de746ba38853.jpg': 0.80995613,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.80a52e224e5f19e1ff83fa270b52ad38.jpg': 0.96603596,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.895f64ba6ddcd0bb6a5177a6d6fd17f0.jpg': 0.9677336,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.9c1fb05ecca1beaef28e980727e1d26f.jpg': 0.955866,\n",
       " '8b3ef1fba766bd5ccebd25fccb4611b5_jpeg_jpg.rf.a3545ee159dc4ddc8fda2d982fa4458b.jpg': 0.7949608,\n",
       " '992_png_jpg.rf.cf705fdb57668dee0a3630f0a98920b6.jpg': 0.97967607,\n",
       " 'a10_mp4-2_jpg.rf.5bb300875bad2a81bc163babce9bd81f.jpg': 0.9290669,\n",
       " 'a10_mp4-6_jpg.rf.52d20d955401a018526850c8d0bd3630.jpg': 0.93259925,\n",
       " 'a11_mp4-1_jpg.rf.d63361ecbad097b7e4c499bbbb20bf0d.jpg': 0.99107033,\n",
       " 'a11_mp4-9_jpg.rf.8d45e305d56e18fb27bf02350bdac614.jpg': 0.99091786,\n",
       " 'a12_mp4-3_jpg.rf.dc3e5d33397baf458c0c02b1ea4472cb.jpg': 0.90399,\n",
       " 'a12_mp4-4_jpg.rf.dbfd49e4d5ec8907d34f27d6ff0d4bfc.jpg': 0.90657616,\n",
       " 'a13_mp4-1_jpg.rf.79abdf81a5b3faf502d385713da2f9ef.jpg': 0.9432528,\n",
       " 'a13_mp4-5_jpg.rf.8d5f0e5ad5a8c1f3520a1540fa94be54.jpg': 0.9738526,\n",
       " 'a14_mp4-6_jpg.rf.fcb9b27894a9c5b149e6cd104f58e72c.jpg': 0.8770706,\n",
       " 'a14_mp4-8_jpg.rf.a6bfed23b4ab2e841d86a7c22ea8e7ad.jpg': 0.89510685,\n",
       " 'a15_mp4-5_jpg.rf.d56833d1c885e619a51ff711732d4de2.jpg': 0.8568385,\n",
       " 'a15_mp4-7_jpg.rf.3e63da3763958ca84c871f3f0f9718f2.jpg': 0.8648948,\n",
       " 'a15_mp4-8_jpg.rf.6b78f6a15e81b740c439e135d5e699de.jpg': 0.9046375,\n",
       " 'a1_mp4-4_jpg.rf.5eeb2c2128d97fd514b9f28cddb36643.jpg': 0.94554794,\n",
       " 'a2_mp4-2_jpg.rf.3be7dd78e8994bc22316022dd9ac7eaf.jpg': 0.85657406,\n",
       " 'a2_mp4-2_jpg.rf.ed63ef5068bc24b4aea26c669d00bf5f.jpg': 0.82963395,\n",
       " 'a2_mp4-3_jpg.rf.06695e177f1e2a55debd8448a885476d.jpg': 0.89516944,\n",
       " 'a2_mp4-3_jpg.rf.3f9f1d45a797bb29315e5c446eab7801.jpg': 0.80225044,\n",
       " 'a2_mp4-3_jpg.rf.65347c9ff8b3de6fa9b5164c9af38b2d.jpg': 0.75045556,\n",
       " 'a2_mp4-4_jpg.rf.409af312e9d21d6fd5a750143521d382.jpg': 0.79155934,\n",
       " 'a2_mp4-5_jpg.rf.479da95db5f0f6475ed4452c0cefc7d2.jpg': 0.89071506,\n",
       " 'a2_mp4-7_jpg.rf.65fb006ec1e9a38a928df8572a811518.jpg': 0.90580547,\n",
       " 'a30eaa7b597bdfebc6e8ef9fffbe1bd8_jpg.rf.e50d31dd839694b3c918163bbd44de20.jpg': 0.8688698,\n",
       " 'a5_mp4-11_jpg.rf.a9d512dd37794e05af961ec759967ae6.jpg': 0.90993196,\n",
       " 'a5_mp4-3_jpg.rf.dd52096bf42e3b0290782268f0b6946f.jpg': 0.93716645,\n",
       " 'a5_mp4-6_jpg.rf.8124ac53fbc3816c9dc1396e2935f538.jpg': 0.93184775,\n",
       " 'a6_mp4-3_jpg.rf.53fc452158b800a2d762a999a6152eac.jpg': 0.9254905,\n",
       " 'a6_mp4-4_jpg.rf.18baaa4d0940bb92853ff066821468ac.jpg': 0.9023866,\n",
       " 'a7_mp4-1_jpg.rf.d8d4d9c6248cfe562a0deef355233c70.jpg': 0.92744255,\n",
       " 'a7_mp4-4_jpg.rf.dfdb289424304013d6fa329464311f10.jpg': 0.92782944,\n",
       " 'a9_mp4-0_jpg.rf.403b0bd9e22ec7f5a6cc906d2ff93dbf.jpg': 0.92016053,\n",
       " 'a9_mp4-5_jpg.rf.38d841ddf312ce9c14f1a5d88bb2852c.jpg': 0.8525,\n",
       " 'abbd7778d558d618979f1d6ab638e196_jpeg_jpg.rf.0510049e212303fd7375ef329a8e79b0.jpg': 0.86123645,\n",
       " 'bde48c4bee65edb5f322e804d881ad2f_jpg.rf.715f6ef5ddf5d7bad75d0b959afabba1.jpg': 0.85679203,\n",
       " 'dc7c651630e5ed76f4f367d1865faee5_jpeg_jpg.rf.635fd25ffd93746f83ef1d9b3281144f.jpg': 0.7635543,\n",
       " 'f2d54bdff146baf7ce10d1e32a6275c5_jpg.rf.9b29030082396249ee2548dc917a630f.jpg': 0.88366425,\n",
       " 'f915b1c365fb1a95bd032798a669fcbf_jpeg_jpg.rf.506a23da3cd0cf724dcf7517634d4661.jpg': 0.9006979,\n",
       " 'f9669e8fe2bd40e1ba0708aa9ff61b4e_jpg.rf.3a3982c6e50dc35a776655cb25eec7c1.jpg': 0.9356519,\n",
       " 'frame_0193_jpg.rf.ae7ded398c75b99f643298b10d1a7f67.jpg': 0.96727073,\n",
       " 'frame_0310_jpg.rf.5bf20a55ebf95d6638c8e52c381f1ab2.jpg': 0.9823176,\n",
       " 'frame_1680_jpg.rf.c81b76866bb6556338de7c90607e06a7.jpg': 0.9607899,\n",
       " 'frame_1901_jpg.rf.5f3df7de3c5545a1cd422be408a34e22.jpg': 0.9642312,\n",
       " 'frame_1904_jpg.rf.583c1bba5fa8773419f6b9cd47e09473.jpg': 0.92155975,\n",
       " 'frame_3260_jpg.rf.fb8a03233c13c2d04a2d77a5768af8f5.jpg': 0.97891873,\n",
       " 'frame_3423_jpg.rf.5c25251fed02cd44fe54091505c030df.jpg': 0.91301674,\n",
       " 'frame_4019_jpg.rf.6a4b9008f4b05d52c98b23e70fa597a2.jpg': 0.9811439,\n",
       " 'frame_4021_jpg.rf.f2f73d7f1de62ff550dc87e59e976629.jpg': 0.98066264,\n",
       " 'frame_4037_jpg.rf.414741be912be1a1266f9d1ec16aa864.jpg': 0.95937383,\n",
       " 'frame_4182_jpg.rf.eb70d5bfd4ce3e51770c804ed32324d0.jpg': 0.9888391,\n",
       " 'frame_4193_jpg.rf.9cd220de53dce389013b45c40a023fe1.jpg': 0.9855693,\n",
       " 'frame_4289_jpg.rf.0b167c20572c0a9cdd685c1cef51b2c2.jpg': 0.97803617,\n",
       " 'frame_4470_jpg.rf.be4bb516f3c83079c697c26eecababd2.jpg': 0.9530084,\n",
       " 'frame_4552_jpg.rf.8cf36449aaee8de6c8be21007d25ee59.jpg': 0.9304954,\n",
       " 'frame_4767_jpg.rf.cc325b95e3e36598ed3c6f51345a79e9.jpg': 0.9390227,\n",
       " 'frame_5255_jpg.rf.b846fdc9b87f77b625a43462574c60ba.jpg': 0.9901686,\n",
       " 'frame_5619_jpg.rf.f4c3a4b3cdca94708b784d87841f1785.jpg': 0.93558717,\n",
       " 'frame_5813_jpg.rf.309dde62aeac7c27f9dc0de2559d487a.jpg': 0.8168601,\n",
       " 'frame_6247_jpg.rf.64429af8d7f7a23b7286735c7f2354db.jpg': 0.94940984,\n",
       " 'frame_7934_jpg.rf.0ceb6e903e7053ed001f5077c175fb0f.jpg': 0.9760278,\n",
       " 'frame_8054_jpg.rf.d3728c6be39d14fed1eb856fc19f0daa.jpg': 0.9613253,\n",
       " 'frame_8177_jpg.rf.4c062517e39e875dfb1505d338c8b219.jpg': 0.8683256,\n",
       " 'frame_m_0268_jpg.rf.2a80ae7075dbac289a4acb4c6a41d52c.jpg': 0.9910063,\n",
       " 'frame_o_7652_jpg.rf.61c2248a62ee0cda17cc508ec5674ec1.jpg': 0.9698047,\n",
       " 'frame_o_7989_jpg.rf.ef795733120eda82f293d6304bc5e7e8.jpg': 0.87484664}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check that data is loaded correctly",
   "id": "22df8eab73d59cfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T11:19:05.322483Z",
     "start_time": "2024-12-08T11:19:05.312917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shape of train_iou:\", len(train_data))\n",
    "print(\"Shape of val_iou:\", len(val_data))"
   ],
   "id": "74440a5c3c99479f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_iou: 1782\n",
      "Shape of val_iou: 187\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute pairwise IoU differences between training and validation set",
   "id": "df69ebc522146a2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for model in ['dlv3', 'dlv3p', 'unet', 'pan']:\n",
    "    train_iou = load_hdf5(f'{root}res/model_performance/train_iou_{model}.h5')['train_iou']\n",
    "    val_iou = load_hdf5(f'{root}res/model_performance/val_iou_{model}.h5')['val_iou']\n",
    "    idx_pair = []\n",
    "    iou_diff = []\n",
    "\n",
    "    for i, t_iou in enumerate(train_iou):\n",
    "        for j, v_iou in enumerate(val_iou):\n",
    "            iou_diff.append(t_iou - v_iou)\n",
    "            idx_pair.append((i, j))\n",
    "\n",
    "    save_to_hdf5(f'{root}res/model_performance/iou_diff_{model}.h5', {'iou_diff': iou_diff, 'idx_pair': idx_pair})\n"
   ],
   "id": "c66b35a3477101c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check that data is saved correctly",
   "id": "3ecdeaadf1def09a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "comp_data = load_hdf5(f'{root}res/model_performance/iou_diff_dlv3.h5')\n",
    "print(\"Shape of iou_diff:\", comp_data['iou_diff'].shape)\n",
    "print(\"Number of pairs:\", len(comp_data['idx_pair']))\n",
    "for i in range(100):\n",
    "    print(f\"Pair {i}: {comp_data['iou_diff'][i+1000]}\")\n",
    "    print(f\"Index pair {i}: {comp_data['idx_pair'][i+1000]}\")"
   ],
   "id": "6184ef31f12ea703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9597468c40527790"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
