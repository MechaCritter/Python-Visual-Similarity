{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "77f0efde90a6ad53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:16:24.251036Z",
     "start_time": "2024-12-15T07:16:18.724819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from sympy.matrices.expressions.tests.test_slice import test_entry\n",
    "from sympy.utilities.tests.test_lambdify import test_integral\n",
    "from torch.utils.data import DataLoader\n",
    "from segmentation_models_pytorch.utils.metrics import IoU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import save_to_hdf5, load_hdf5, load_model, multiclass_iou\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.config import IMAGE_SIZE, TRANSFORMER, DEVICE, ROOT\n",
    "from models.Segmentation import DeepLabV3Model, DeepLabV3PlusModel, PyramidAttentionNetworkModel, UNetModel"
   ],
   "id": "2f7ce4361f2319d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Root",
   "id": "4e329e2ecdbe880b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:16:24.268628Z",
     "start_time": "2024-12-15T07:16:24.265094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = ROOT\n",
    "batch_size = 1"
   ],
   "id": "55703a431c275fda",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Initialize models\n",
    "\n",
    "**Note**: UNEt performs quite badly (only achieves `val IoU` of 0.78)."
   ],
   "id": "d4de6564b6b60cda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:16:25.867964Z",
     "start_time": "2024-12-15T07:16:24.322666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DeepLabV3\n",
    "dlv3 =DeepLabV3Model(model_path=f'{ROOT}/models/torch_model_files/DeepLabV3_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# DeepLabV3Plus\n",
    "dlv3p = DeepLabV3PlusModel(model_path=f'{ROOT}/models/torch_model_files/DeepLabV3Plus_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# UNet\n",
    "unet = UNetModel(model_path=f'{ROOT}/models/torch_model_files/UNet_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# Pyramid Attention Network\n",
    "pan = PyramidAttentionNetworkModel(model_path=f'{ROOT}/models/torch_model_files/PyramidAttentionNetwork_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# Pass a model to its corresponding ioU file\n",
    "iou_paths = {\n",
    "    dlv3: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_dlv3.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_dlv3.h5'\n",
    "    },\n",
    "    dlv3p: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_dlv3p.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_dlv3p.h5'\n",
    "    },\n",
    "    unet: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_unet.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_unet.h5'\n",
    "    },\n",
    "    pan: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_pan.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_pan.h5'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Confidence paths\n",
    "confidence_paths = {\n",
    "    dlv3: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3_no_bg_val.h5'\n",
    "        }\n",
    "    },\n",
    "    dlv3p: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3p_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3p_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3p_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3p_no_bg_val.h5'\n",
    "        }\n",
    "    },\n",
    "    unet: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/unet_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/unet_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/unet_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/unet_no_bg_val.h5'\n",
    "        }\n",
    "    },\n",
    "    pan: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/pan_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/pan_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/pan_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/pan_no_bg_val.h5'\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "1f2f78cb5a2d7112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 08:16:24,864 - DeepLabV3 - INFO - Device used for model: cuda\n",
      "2024-12-15 08:16:25,178 - DeepLabV3Plus - INFO - Device used for model: cuda\n",
      "2024-12-15 08:16:25,515 - UNet - INFO - Device used for model: cuda\n",
      "2024-12-15 08:16:25,833 - PyramidAttentionNetwork - INFO - Device used for model: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "92050708a12968bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:16:27.009287Z",
     "start_time": "2024-12-15T07:16:26.988736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train', transform=TRANSFORMER)\n",
    "print(\"Number of training samples:\", num_train_imgs:=len(train_dataset))\n",
    "test_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test', transform=TRANSFORMER)\n",
    "print(\"Number of test samples:\", num_test_imgs:=len(test_dataset))"
   ],
   "id": "179bd9fc0e4693c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1782\n",
      "Number of test samples: 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/Bachelorarbeit/similarity_metrics_of_images/src/datasets.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(value / 255.0, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save predicted masks",
   "id": "1022b1e7e9891b8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:30:59.802658Z",
     "start_time": "2024-12-12T19:30:59.796101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_iou_dlv3 = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_dlv3p = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_unet = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_pan = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_paths = []\n",
    "\n",
    "test_iou_dlv3 = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_dlv3p = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_unet = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_pan = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_paths = []"
   ],
   "id": "84234215ecff9ec5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:32:01.650192Z",
     "start_time": "2024-12-12T19:31:01.576382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compute predicted masks for training set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(train_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    train_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    train_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    train_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    train_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    train_paths.append(paths)\n",
    "\n",
    "train_paths = [path.replace('|', '/') for path in train_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_pan)})\n",
    "\n",
    "# Compute predicted masks for validation set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(test_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    test_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    test_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    test_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    test_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    test_paths.append(paths)\n",
    "\n",
    "test_paths = [path.replace('|', '/') for path in test_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_pan)})\n"
   ],
   "id": "1f88383ddd878368",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1782it [00:53, 33.43it/s]\n",
      "187it [00:05, 34.98it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save Prediction Confidence for all models",
   "id": "63e6a192e4aa385e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:26:46.941700Z",
     "start_time": "2024-12-15T07:20:31.385118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_and_save_confidence_vectors(model, train_dataset, val_dataset, iou_paths):\n",
    "    \"\"\"\n",
    "    Compute and save prediction probability vectors for train and val sets.\n",
    "    \"\"\"\n",
    "    model_name = model.model.__class__.__name__\n",
    "    out_paths = iou_paths[model]  # {'train': ..., 'val': ...}\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(out_paths['with_background']['train']), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_paths['with_background']['val']), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_paths['no_background']['train']), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_paths['no_background']['val']), exist_ok=True)\n",
    "\n",
    "    # We'll save dictionaries first, then write to HDF5\n",
    "    train_results_with_bg = {}\n",
    "    train_results_no_bg = {}\n",
    "    val_results_with_bg = {}\n",
    "    val_results_no_bg = {}\n",
    "\n",
    "    # Process Train Set\n",
    "    for img, mask, path in tqdm(train_dataset, desc=f\"Computing confidence vectors (Train) for {model_name}\"):\n",
    "        # Get probabilities with background\n",
    "        conf_with_bg, _ = model.predict_single_image(img, mask,                                          return_raw_prob_vector=True,                                             ignore_background=False)\n",
    "        # Get probabilities without background\n",
    "        conf_no_bg, _ = model.predict_single_image(img, mask,\n",
    "                                                   return_raw_prob_vector=True,\n",
    "                                                   ignore_background=True)\n",
    "        train_results_with_bg[os.path.basename(path)] = conf_with_bg.cpu().numpy()\n",
    "        train_results_no_bg[os.path.basename(path)] = conf_no_bg.cpu().numpy()\n",
    "\n",
    "    # Process Val Set\n",
    "    for img, mask, path in tqdm(val_dataset, desc=f\"Computing confidence vectors (Val) for {model_name}\"):\n",
    "        # Get probabilities with background\n",
    "        conf_with_bg, _ = model.predict_single_image(img, mask,\n",
    "                                                     return_raw_prob_vector=True,\n",
    "                                                     ignore_background=False)\n",
    "        # Get probabilities without background\n",
    "        conf_no_bg, _ = model.predict_single_image(img, mask,\n",
    "                                                   return_raw_prob_vector=True,\n",
    "                                                   ignore_background=True)\n",
    "        val_results_with_bg[os.path.basename(path)] = conf_with_bg.cpu().numpy()\n",
    "        val_results_no_bg[os.path.basename(path)] = conf_no_bg.cpu().numpy()\n",
    "\n",
    "    # Save results\n",
    "    save_to_hdf5(out_paths['with_background']['train'], train_results_with_bg)\n",
    "    save_to_hdf5(out_paths['with_background']['val'], val_results_with_bg)\n",
    "    save_to_hdf5(out_paths['no_background']['train'], train_results_no_bg)\n",
    "    save_to_hdf5(out_paths['no_background']['val'], val_results_no_bg)\n",
    "\n",
    "# Run for all models\n",
    "compute_and_save_confidence_vectors(dlv3, train_dataset, test_dataset, confidence_paths)\n",
    "compute_and_save_confidence_vectors(dlv3p, train_dataset, test_dataset, confidence_paths)\n",
    "compute_and_save_confidence_vectors(unet, train_dataset, test_dataset, confidence_paths)\n",
    "compute_and_save_confidence_vectors(pan, train_dataset, test_dataset, confidence_paths)"
   ],
   "id": "6c1d95223a246fd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing confidence vectors (Train) for DeepLabV3:   0%|          | 0/1782 [00:00<?, ?it/s]/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "Computing confidence vectors (Train) for DeepLabV3: 100%|██████████| 1782/1782 [01:24<00:00, 21.12it/s]\n",
      "Computing confidence vectors (Val) for DeepLabV3: 100%|██████████| 187/187 [00:08<00:00, 21.11it/s]\n",
      "Computing confidence vectors (Train) for DeepLabV3Plus: 100%|██████████| 1782/1782 [01:21<00:00, 21.94it/s]\n",
      "Computing confidence vectors (Val) for DeepLabV3Plus: 100%|██████████| 187/187 [00:07<00:00, 24.37it/s]\n",
      "Computing confidence vectors (Train) for Unet: 100%|██████████| 1782/1782 [01:31<00:00, 19.46it/s]\n",
      "Computing confidence vectors (Val) for Unet: 100%|██████████| 187/187 [00:09<00:00, 20.17it/s]\n",
      "Computing confidence vectors (Train) for PAN: 100%|██████████| 1782/1782 [01:22<00:00, 21.59it/s]\n",
      "Computing confidence vectors (Val) for PAN: 100%|██████████| 187/187 [00:08<00:00, 21.01it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check that data is loaded correctly",
   "id": "22df8eab73d59cfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:34:18.774400Z",
     "start_time": "2024-12-12T19:34:18.767595Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "74440a5c3c99479f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_iou: 1782\n",
      "Shape of val_iou: 187\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9597468c40527790"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
