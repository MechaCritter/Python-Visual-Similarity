{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "77f0efde90a6ad53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from sympy.matrices.expressions.tests.test_slice import test_entry\n",
    "from sympy.utilities.tests.test_lambdify import test_integral\n",
    "from torch.utils.data import DataLoader\n",
    "from segmentation_models_pytorch.utils.metrics import IoU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import save_to_hdf5, load_hdf5, load_model, multiclass_iou\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.config import IMAGE_SIZE, TRANSFORMER, DEVICE, ROOT\n",
    "from src.evaluate import compute_and_save_confidence_vectors\n",
    "from models.Segmentation import DeepLabV3Model, DeepLabV3PlusModel, PyramidAttentionNetworkModel, UNetModel"
   ],
   "id": "2f7ce4361f2319d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Root",
   "id": "4e329e2ecdbe880b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "root = ROOT\n",
    "batch_size = 1"
   ],
   "id": "55703a431c275fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Initialize models\n",
    "\n",
    "**Note**: UNEt performs quite badly (only achieves `val IoU` of 0.78)."
   ],
   "id": "d4de6564b6b60cda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DeepLabV3\n",
    "dlv3 =DeepLabV3Model(model_path=f'{ROOT}/models/torch_model_files/DeepLabV3_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# DeepLabV3Plus\n",
    "dlv3p = DeepLabV3PlusModel(model_path=f'{ROOT}/models/torch_model_files/DeepLabV3Plus_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# UNet\n",
    "unet = UNetModel(model_path=f'{ROOT}/models/torch_model_files/UNet_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# Pyramid Attention Network\n",
    "pan = PyramidAttentionNetworkModel(model_path=f'{ROOT}/models/torch_model_files/PyramidAttentionNetwork_HybridFocalDiceLoss.pt')\n",
    "\n",
    "# Pass a model to its corresponding ioU file\n",
    "iou_paths = {\n",
    "    dlv3: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_dlv3.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_dlv3.h5'\n",
    "    },\n",
    "    dlv3p: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_dlv3p.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_dlv3p.h5'\n",
    "    },\n",
    "    unet: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_unet.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_unet.h5'\n",
    "    },\n",
    "    pan: {\n",
    "        'train': f'{ROOT}/res/model_performance/train_iou_pan.h5',\n",
    "        'val': f'{ROOT}/res/model_performance/val_iou_pan.h5'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Confidence paths\n",
    "confidence_paths = {\n",
    "    dlv3: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3_no_bg_val.h5'\n",
    "        },\n",
    "        'only_classes_in_train': f'{ROOT}/res/confidence_vectors/dlv3_only_classes_in_train.h5'\n",
    "    },\n",
    "    dlv3p: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3p_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3p_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/dlv3p_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/dlv3p_no_bg_val.h5'\n",
    "        },\n",
    "        'only_classes_in_train': f'{ROOT}/res/confidence_vectors/dlv3p_only_classes_in_train.h5'\n",
    "    },\n",
    "    unet: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/unet_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/unet_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/unet_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/unet_no_bg_val.h5'\n",
    "        },\n",
    "        'only_classes_in_train': f'{ROOT}/res/confidence_vectors/unet_only_classes_in_train.h5'\n",
    "    },\n",
    "    pan: {\n",
    "        'with_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/pan_with_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/pan_with_bg_val.h5'\n",
    "        },\n",
    "        'no_background': {\n",
    "            'train': f'{ROOT}/res/confidence_vectors/pan_no_bg_train.h5',\n",
    "            'val': f'{ROOT}/res/confidence_vectors/pan_no_bg_val.h5'\n",
    "        },\n",
    "        'only_classes_in_train': f'{ROOT}/res/confidence_vectors/pan_only_classes_in_train.h5'\n",
    "    }\n",
    "}"
   ],
   "id": "1f2f78cb5a2d7112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "92050708a12968bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train', transform=TRANSFORMER)\n",
    "print(\"Number of training samples:\", num_train_imgs:=len(train_dataset))\n",
    "test_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test', transform=TRANSFORMER)\n",
    "print(\"Number of test samples:\", num_test_imgs:=len(test_dataset))"
   ],
   "id": "179bd9fc0e4693c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save predicted masks",
   "id": "1022b1e7e9891b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_iou_dlv3 = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_dlv3p = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_unet = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_iou_pan = torch.zeros(num_train_imgs, dtype=torch.float32, device=DEVICE)\n",
    "train_paths = []\n",
    "\n",
    "test_iou_dlv3 = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_dlv3p = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_unet = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_iou_pan = torch.zeros(num_test_imgs, dtype=torch.float32, device=DEVICE)\n",
    "test_paths = []"
   ],
   "id": "84234215ecff9ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Compute predicted masks for training set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(train_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    train_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    train_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    train_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    train_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    train_paths.append(paths)\n",
    "\n",
    "train_paths = [path.replace('|', '/') for path in train_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/train_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(train_paths, train_iou_pan)})\n",
    "\n",
    "# Compute predicted masks for validation set\n",
    "for i, (imgs, masks, paths) in tqdm(enumerate(test_dataset)):\n",
    "    imgs = imgs.to(DEVICE).unsqueeze(0)\n",
    "    masks = masks.to(DEVICE)\n",
    "    output_dlv3 = dlv3(imgs).squeeze(0)\n",
    "    output_dlv3p = dlv3p(imgs).squeeze(0)\n",
    "    output_unet = unet(imgs).squeeze(0)\n",
    "    output_pan = pan(imgs).squeeze(0)\n",
    "    test_iou_dlv3[i] = multiclass_iou(output_dlv3, masks)\n",
    "    test_iou_dlv3p[i] = multiclass_iou(output_dlv3p, masks)\n",
    "    test_iou_unet[i] = multiclass_iou(output_unet, masks)\n",
    "    test_iou_pan[i] = multiclass_iou(output_pan, masks)\n",
    "    test_paths.append(paths)\n",
    "\n",
    "test_paths = [path.replace('|', '/') for path in test_paths]\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_dlv3)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_dlv3p.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_dlv3p)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_unet.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_unet)})\n",
    "save_to_hdf5(f'{root}/res/model_performance/val_iou_pan.h5', {os.path.basename(pths): iou.cpu().numpy() for pths, iou in zip(test_paths, test_iou_pan)})\n"
   ],
   "id": "1f88383ddd878368",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save Prediction Confidence for all models",
   "id": "63e6a192e4aa385e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A) All classes are considered, background included",
   "id": "34af076abdb0a30b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compute_and_save_confidence_vectors(dlv3,\n",
    "                                    train_dataset,\n",
    "                                    output_dir=confidence_paths[dlv3]['with_background']['train'])\n",
    "compute_and_save_confidence_vectors(dlv3p,\n",
    "                                    train_dataset,\n",
    "                                    output_dir=confidence_paths[dlv3p]['with_background']['train'])\n",
    "compute_and_save_confidence_vectors(unet,\n",
    "                                    train_dataset,\n",
    "                                    output_dir=confidence_paths[unet]['with_background']['train'])\n",
    "compute_and_save_confidence_vectors(pan,\n",
    "                                    train_dataset,\n",
    "                                    output_dir=confidence_paths[pan]['with_background']['train'])\n",
    "\n",
    "compute_and_save_confidence_vectors(dlv3,\n",
    "                                    test_dataset,\n",
    "                                    output_dir=confidence_paths[dlv3]['with_background']['val'])\n",
    "compute_and_save_confidence_vectors(dlv3p,\n",
    "                                    test_dataset,\n",
    "                                    output_dir=confidence_paths[dlv3p]['with_background']['val'])\n",
    "compute_and_save_confidence_vectors(unet,\n",
    "                                    test_dataset,\n",
    "                                    output_dir=confidence_paths[unet]['with_background']['val'])\n",
    "compute_and_save_confidence_vectors(pan,\n",
    "                                    test_dataset,\n",
    "                                    output_dir=confidence_paths[pan]['with_background']['val'])"
   ],
   "id": "6c1d95223a246fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## B) All classes are considered, background excluded",
   "id": "22df8eab73d59cfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compute_and_save_confidence_vectors(dlv3,\n",
    "                                    train_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[dlv3]['no_background']['train'])\n",
    "compute_and_save_confidence_vectors(dlv3p,\n",
    "                                    train_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[dlv3p]['no_background']['train'])\n",
    "compute_and_save_confidence_vectors(unet,\n",
    "                                    train_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[unet]['no_background']['train'])\n",
    "compute_and_save_confidence_vectors(pan,\n",
    "                                    train_dataset,\n",
    "                                    output_dir=confidence_paths[pan]['no_background']['train'])\n",
    "\n",
    "compute_and_save_confidence_vectors(dlv3,\n",
    "                                    test_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[dlv3]['no_background']['val'])\n",
    "compute_and_save_confidence_vectors(dlv3p,\n",
    "                                    test_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[dlv3p]['no_background']['val'])\n",
    "compute_and_save_confidence_vectors(unet,\n",
    "                                    test_dataset,\n",
    "                                    output_dir=confidence_paths[unet]['no_background']['val'])\n",
    "compute_and_save_confidence_vectors(pan,\n",
    "                                    test_dataset,\n",
    "                                    ignore_background=True,\n",
    "                                    output_dir=confidence_paths[pan]['no_background']['val'])"
   ],
   "id": "74440a5c3c99479f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## C) Only classes in the training masks are considered",
   "id": "86edcb0e52900a9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compute_and_save_confidence_vectors(dlv3,\n",
    "                                    train_dataset,\n",
    "                                    only_classes_in_gt_mask=True,\n",
    "                                    output_dir=confidence_paths[dlv3]['only_classes_in_train'])\n",
    "compute_and_save_confidence_vectors(dlv3p,\n",
    "                                    train_dataset,\n",
    "                                    only_classes_in_gt_mask=True,\n",
    "                                    output_dir=confidence_paths[dlv3p]['only_classes_in_train'])\n",
    "compute_and_save_confidence_vectors(unet,\n",
    "                                    train_dataset,\n",
    "                                    only_classes_in_gt_mask=True,\n",
    "                                    output_dir=confidence_paths[unet]['only_classes_in_train'])\n",
    "compute_and_save_confidence_vectors(pan,\n",
    "                                    train_dataset,\n",
    "                                    only_classes_in_gt_mask=True,\n",
    "                                    output_dir=confidence_paths[pan]['only_classes_in_train'])"
   ],
   "id": "9597468c40527790",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
