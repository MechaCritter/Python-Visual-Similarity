{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# For each of the metrics: VLAD, Fisher, SSIM, MS-SSIM:\n",
    "\n",
    "- Load the pre-computed data (if any) or compute on the fly\n",
    "- Cluster the images based on the computed data"
   ],
   "id": "c222bcf042165b89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Import libraries",
   "id": "72e237d2f1068b71"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T13:02:25.945983Z",
     "start_time": "2024-12-19T13:02:15.543008Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Use Agg backend to save figures\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from src.utils import *\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.metrics import VLAD, FisherVector\n",
    "from src.config import ROOT"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:02:25.956919Z",
     "start_time": "2024-12-19T13:02:25.951455Z"
    }
   },
   "cell_type": "code",
   "source": "root = ROOT",
   "id": "5160cd05b0fcf973",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:02:26.147979Z",
     "start_time": "2024-12-19T13:02:26.079158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train')\n",
    "val_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test')"
   ],
   "id": "bc4525816618e487",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/Bachelorarbeit/similarity_metrics_of_images/src/datasets.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(value / 255.0, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their VLAD/Fisher vectors (DONE)",
   "id": "66113a295ff664d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import *\n",
    "import tqdm\n",
    "\n",
    "k_means_models = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'k_means' in model\n",
    "]\n",
    "\n",
    "\n",
    "vectors_dir = f'{root}/res/vlad'\n",
    "output_dir = f'{root}/res/similarity_matrix'\n",
    "\n",
    "for num_clusters in range(10, 61, 5):\n",
    "    for model in k_means_models:\n",
    "        # h5_file_name = model.replace('.pkl', '.h5')\n",
    "        # data = load_hdf5(f'{root}/res/vlad/train/{h5_file_name}')\n",
    "        # image_paths = list(data.keys())\n",
    "        # vectors = np.array([data[path] for path in image_paths])\n",
    "        # image_paths = [f'{root}/excavator_dataset_w_masks/{path}' for path in image_paths]\n",
    "        num_cls = int(re.findall(r'\\d+', model)[0])\n",
    "        vectors = np.empty((0, 128 * num_cls))\n",
    "        image_paths = []\n",
    "        for img, *_, path in train_dataset:\n",
    "            vectors = np.append(vectors, VLAD(\n",
    "                image=img,\n",
    "                k_means=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "                flatten=True,\n",
    "                feature='root_sift' if 'root_sift' in model else 'sift'\n",
    "            ).vector.reshape(1, -1), axis=0)\n",
    "            image_paths.append(path)\n",
    "\n",
    "        cluster_images_and_save(image_paths,\n",
    "                                vectors,\n",
    "                                n_clusters=num_clusters,\n",
    "                                output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/vlad/{model}')"
   ],
   "id": "89ffd51ff07c91a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their Fisher vectors (DONE)",
   "id": "3217421223763e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gmm_models = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'gmm' in model\n",
    "]\n",
    "\n",
    "vectors_dir = f'{root}/res/fisher'\n",
    "output_dir = f'{root}/res/similarity_matrix'\n",
    "\n",
    "for num_clusters in range(10, 61, 5):\n",
    "    for model in gmm_models:\n",
    "        num_cls = int(re.findall(r'\\d+', model)[0])\n",
    "        vectors = np.empty((0, 2 * 128 * num_cls + num_cls))\n",
    "        image_paths = []\n",
    "\n",
    "        for img, *_, path in train_dataset:\n",
    "            vectors = np.append(vectors, FisherVector(\n",
    "                image=img,\n",
    "                gmm=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "                flatten=True,\n",
    "                feature='root_sift' if 'root_sift' in model else 'sift'\n",
    "            ).vector.reshape(1, -1), axis=0)\n",
    "            image_paths.append(path)\n",
    "        cluster_images_and_save(image_paths,\n",
    "                                vectors,\n",
    "                                n_clusters=num_clusters,\n",
    "                                output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/fisher/{model}')"
   ],
   "id": "49a6bbffa28d44e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their SSIM and MS-SSIM values (TODO)",
   "id": "d83554488eff7d0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SSIM and MS-SSIM data had to be pre-computed and saved as hdf5 files because the computation was horrifyingly slow and intensive. Problem is, file paths were saved as basenames only and not full paths, so the matrix has to be sorted again to the correct order.",
   "id": "57b9a59382d7b478"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:12:31.688339Z",
     "start_time": "2024-12-20T08:12:19.001818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SORTED_PATHS = {os.path.basename(path): i for i, (*_, path) in enumerate(train_dataset)}\n",
    "SORTED_FULLPATHS = {path: i for i, (*_, path) in enumerate(train_dataset)}\n",
    "\n",
    "def sort_ssim_data(ssim_data: dict[str, dict[str, np.ndarray, np.ndarray]]) -> dict[str, dict[str, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Sorts the 'ssim' and 'ms_ssim' matrices in ssim_data according to the correct order\n",
    "    defined by the 'sorted_paths'. It reorders both rows and columns so that the order\n",
    "    corresponds to the indexes provided by the sorted_paths.\n",
    "\n",
    "    :param ssim_data:\n",
    "        A dictionary that should contain:\n",
    "        'train_paths' : list of str representing the baseline names.\n",
    "        'ssim'        : np.ndarray of shape (N, N)\n",
    "        'ms_ssim'     : np.ndarray of shape (N, N)\n",
    "        'sorted_paths': list of tuples (idx, full_path) representing the correct order.\n",
    "\n",
    "    :returns:\n",
    "        Updated ssim_data with reordered 'ssim' and 'ms_ssim' matrices.\n",
    "    \"\"\"\n",
    "    train_paths = ssim_data['image_paths']\n",
    "    ssim = ssim_data['ssim']\n",
    "    ms_ssim = ssim_data['ms_ssim']\n",
    "    num_train = len(train_paths)\n",
    "    if ssim.shape[0] != num_train or ssim.shape[1] != num_train:\n",
    "        raise ValueError(\"'ssim' matrix shape does not match the number of train paths.\")\n",
    "    if ms_ssim.shape[0] != num_train or ms_ssim.shape[1] != num_train:\n",
    "        raise ValueError(\"'ms_ssim' matrix shape does not match the number of train paths.\")\n",
    "\n",
    "    new_order = [SORTED_PATHS[basename.decode('utf-8')] for basename in train_paths] # Hash table lookup x loop  -> O(N) * O(1) = O(N)\n",
    "    sorted_paths = train_paths[new_order]\n",
    "    ssim = ssim[new_order][:, new_order]\n",
    "    ms_ssim = ms_ssim[new_order][:, new_order]\n",
    "    sorted_data = {'image_paths': sorted_paths,'ssim': ssim,'ms_ssim': ms_ssim}\n",
    "    return sorted_data"
   ],
   "id": "2e4bf97a8feff038",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cluster images based on SSIM and MS-SSIM values",
   "id": "2c9c6fb1817fbf18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:12:38.337006Z",
     "start_time": "2024-12-20T08:12:34.599861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for num_clusters in range(15, 61, 5):\n",
    "    for sigma in range(0, 12, 2):\n",
    "        hdf5_path = f'{root}/res/ssim/within_train/ssim_sigma{sigma}.h5'\n",
    "        ssim_data = load_hdf5(hdf5_path)\n",
    "        sorted_ssims = sort_ssim_data(ssim_data)\n",
    "        sorted_paths = sorted_ssims['image_paths']\n",
    "        for feat in ['ssim', 'ms_ssim']:\n",
    "            data = ssim_data[feat]\n",
    "            row, col = data.shape\n",
    "            # SSIm and MS_SSIM matrices should be square with shape (N, N)\n",
    "            if row != len(num_imgs := list(SORTED_PATHS.keys())) or col != len(num_imgs):\n",
    "                raise ValueError(f\"Expected shape ({len(num_imgs)}, {len(num_imgs)}), got ({row}, {col})\")\n",
    "            cluster_images_and_save(list(SORTED_FULLPATHS.keys()),\n",
    "                                    data,\n",
    "                                    n_clusters=num_clusters,\n",
    "                                    output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/ssim/{feat}_sigma{sigma}')"
   ],
   "id": "7cfb4d9aaf6d91fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:12:34,685 - root - INFO - Clustering 1782 images into 15 clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters:   0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m row \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(num_imgs \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mlist\u001B[39m(SORTED_PATHS\u001B[38;5;241m.\u001B[39mkeys())) \u001B[38;5;129;01mor\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(num_imgs):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected shape (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(num_imgs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(num_imgs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m), got (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[43mcluster_images_and_save\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mSORTED_PATHS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m                        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mroot\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/res/clustered_datasets_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mnum_clusters\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_clusters/ssim/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfeat\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_sigma\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msigma\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:541\u001B[0m, in \u001B[0;36mcluster_images_and_save\u001B[0;34m(image_paths, features, n_clusters, output_dir, generate_heatmap, heatmap_title, rename_images, verbose)\u001B[0m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     plot_and_save_heatmap(\n\u001B[1;32m    533\u001B[0m         matrix\u001B[38;5;241m=\u001B[39msimilarity_matrix,\n\u001B[1;32m    534\u001B[0m         title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheatmap_title\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Cluster \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_below_diagonal(similarity_matrix)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Avr Similarity\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    538\u001B[0m         save_fig_path\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(cluster_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheatmap_cluster_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    539\u001B[0m         show\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    540\u001B[0m     plot_and_save_heatmap(\n\u001B[0;32m--> 541\u001B[0m         matrix\u001B[38;5;241m=\u001B[39meuclidean_similarity_matrix,\n\u001B[1;32m    542\u001B[0m         title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheatmap_title\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Cluster \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_below_diagonal(euclidean_similarity_matrix)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Avr Euclidean Similarity\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    543\u001B[0m         x_tick_labels\u001B[38;5;241m=\u001B[39mcluster_index_list,\n\u001B[1;32m    544\u001B[0m         y_tick_labels\u001B[38;5;241m=\u001B[39mcluster_index_list,\n\u001B[1;32m    545\u001B[0m         cbar_kws\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEuclidean Similarity\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    546\u001B[0m         save_fig_path\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(cluster_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meuclidean_similarity_cluster_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    547\u001B[0m         show\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    549\u001B[0m     logging\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError generating heatmap for cluster \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Probably the heatmap is too large.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m<frozen posixpath>:118\u001B[0m, in \u001B[0;36msplitext\u001B[0;34m(p)\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected str, bytes or os.PathLike object, not int"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b173148f6acbc781"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fecff2740e074448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab38073d1892e4ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
