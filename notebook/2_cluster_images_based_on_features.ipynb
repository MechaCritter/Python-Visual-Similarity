{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# For each of the metrics: VLAD, Fisher, SSIM, MS-SSIM:\n",
    "\n",
    "- Load the pre-computed data (if any) or compute on the fly\n",
    "- Cluster the images based on the computed data"
   ],
   "id": "c222bcf042165b89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Import libraries",
   "id": "72e237d2f1068b71"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Use Agg backend to save figures\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from src.utils import *\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.metrics import VLAD, FisherVector\n",
    "from src.config import ROOT"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "root = ROOT",
   "id": "5160cd05b0fcf973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train')\n",
    "val_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test')"
   ],
   "id": "bc4525816618e487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their VLAD/Fisher vectors (DONE)",
   "id": "66113a295ff664d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import *\n",
    "import tqdm\n",
    "\n",
    "k_means_models = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'k_means' in model\n",
    "]\n",
    "\n",
    "\n",
    "vectors_dir = f'{root}/res/vlad'\n",
    "output_dir = f'{root}/res/similarity_matrix'\n",
    "\n",
    "for num_clusters in range(10, 61, 5):\n",
    "    for model in k_means_models:\n",
    "        # h5_file_name = model.replace('.pkl', '.h5')\n",
    "        # data = load_hdf5(f'{root}/res/vlad/train/{h5_file_name}')\n",
    "        # image_paths = list(data.keys())\n",
    "        # vectors = np.array([data[path] for path in image_paths])\n",
    "        # image_paths = [f'{root}/excavator_dataset_w_masks/{path}' for path in image_paths]\n",
    "        num_cls = int(re.findall(r'\\d+', model)[0])\n",
    "        vectors = np.empty((0, 128 * num_cls))\n",
    "        image_paths = []\n",
    "        for img, *_, path in train_dataset:\n",
    "            vectors = np.append(vectors, VLAD(\n",
    "                image=img,\n",
    "                k_means=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "                flatten=True,\n",
    "                feature='root_sift' if 'root_sift' in model else 'sift'\n",
    "            ).vector.reshape(1, -1), axis=0)\n",
    "            image_paths.append(path)\n",
    "\n",
    "        cluster_images_and_save(image_paths,\n",
    "                                vectors,\n",
    "                                n_clusters=num_clusters,\n",
    "                                output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/vlad/{model}')"
   ],
   "id": "89ffd51ff07c91a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their Fisher vectors (DONE)",
   "id": "3217421223763e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gmm_models = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'gmm' in model\n",
    "]\n",
    "\n",
    "vectors_dir = f'{root}/res/fisher'\n",
    "output_dir = f'{root}/res/similarity_matrix'\n",
    "\n",
    "for num_clusters in range(10, 61, 5):\n",
    "    for model in gmm_models:\n",
    "        num_cls = int(re.findall(r'\\d+', model)[0])\n",
    "        vectors = np.empty((0, 2 * 128 * num_cls + num_cls))\n",
    "        image_paths = []\n",
    "\n",
    "        for img, *_, path in train_dataset:\n",
    "            vectors = np.append(vectors, FisherVector(\n",
    "                image=img,\n",
    "                gmm=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "                flatten=True,\n",
    "                feature='root_sift' if 'root_sift' in model else 'sift'\n",
    "            ).vector.reshape(1, -1), axis=0)\n",
    "            image_paths.append(path)\n",
    "        cluster_images_and_save(image_paths,\n",
    "                                vectors,\n",
    "                                n_clusters=num_clusters,\n",
    "                                output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/fisher/{model}')"
   ],
   "id": "49a6bbffa28d44e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster images based on their SSIM and MS-SSIM values (DONE)",
   "id": "d83554488eff7d0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SSIM and MS-SSIM data had to be pre-computed and saved as hdf5 files because the computation was horrifyingly slow and intensive. Problem is, file paths were saved as basenames only and not full paths, so the matrix has to be sorted again to the correct order.",
   "id": "57b9a59382d7b478"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SORTED_PATHS = {os.path.basename(path): i for i, (*_, path) in enumerate(train_dataset)}\n",
    "SORTED_FULLPATHS = {path: i for i, (*_, path) in enumerate(train_dataset)}\n",
    "\n",
    "def sort_ssim_data(ssim_data: dict[str, dict[str, np.ndarray, np.ndarray]]) -> dict[str, dict[str, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Sorts the 'ssim' and 'ms_ssim' matrices in ssim_data according to the correct order\n",
    "    defined by the 'sorted_paths'. It reorders both rows and columns so that the order\n",
    "    corresponds to the indexes provided by the sorted_paths.\n",
    "\n",
    "    :param ssim_data:\n",
    "        A dictionary that should contain:\n",
    "        'train_paths' : list of str representing the baseline names.\n",
    "        'ssim'        : np.ndarray of shape (N, N)\n",
    "        'ms_ssim'     : np.ndarray of shape (N, N)\n",
    "        'sorted_paths': list of tuples (idx, full_path) representing the correct order.\n",
    "\n",
    "    :returns:\n",
    "        Updated ssim_data with reordered 'ssim' and 'ms_ssim' matrices.\n",
    "    \"\"\"\n",
    "    train_paths = ssim_data['image_paths']\n",
    "    ssim = ssim_data['ssim']\n",
    "    ms_ssim = ssim_data['ms_ssim']\n",
    "    num_train = len(train_paths)\n",
    "    if ssim.shape[0] != num_train or ssim.shape[1] != num_train:\n",
    "        raise ValueError(\"'ssim' matrix shape does not match the number of train paths.\")\n",
    "    if ms_ssim.shape[0] != num_train or ms_ssim.shape[1] != num_train:\n",
    "        raise ValueError(\"'ms_ssim' matrix shape does not match the number of train paths.\")\n",
    "\n",
    "    new_order = [SORTED_PATHS[basename.decode('utf-8')] for basename in train_paths] # Hash table lookup x loop  -> O(N) * O(1) = O(N)\n",
    "    sorted_paths = train_paths[new_order]\n",
    "    ssim = ssim[new_order][:, new_order]\n",
    "    ms_ssim = ms_ssim[new_order][:, new_order]\n",
    "    sorted_data = {'image_paths': sorted_paths,'ssim': ssim,'ms_ssim': ms_ssim}\n",
    "    return sorted_data"
   ],
   "id": "2e4bf97a8feff038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for num_clusters in range(15, 61, 5):\n",
    "    for sigma in range(0, 12, 2):\n",
    "        hdf5_path = f'{root}/res/ssim/within_train/ssim_sigma{sigma}.h5'\n",
    "        ssim_data = load_hdf5(hdf5_path)\n",
    "        sorted_ssims = sort_ssim_data(ssim_data)\n",
    "        sorted_paths = sorted_ssims['image_paths']\n",
    "        for feat in ['ssim', 'ms_ssim']:\n",
    "            data = ssim_data[feat]\n",
    "            row, col = data.shape\n",
    "            # SSIm and MS_SSIM matrices should be square with shape (N, N)\n",
    "            if row != len(num_imgs := list(SORTED_PATHS.keys())) or col != len(num_imgs):\n",
    "                raise ValueError(f\"Expected shape ({len(num_imgs)}, {len(num_imgs)}), got ({row}, {col})\")\n",
    "            cluster_images_and_save(list(SORTED_FULLPATHS.keys()),\n",
    "                                    data,\n",
    "                                    n_clusters=num_clusters,\n",
    "                                    output_dir=f'{root}/res/clustered_datasets_{num_clusters}_clusters/ssim/{feat}_sigma{sigma}')"
   ],
   "id": "7cfb4d9aaf6d91fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b173148f6acbc781"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fecff2740e074448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab38073d1892e4ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
