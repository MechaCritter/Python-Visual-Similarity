{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "50ea7da99a18c522"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T19:28:49.835568Z",
     "start_time": "2024-12-18T19:28:38.253882Z"
    }
   },
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import h5py\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.utils import *\n",
    "from src.datasets import ExcavatorDataset\n",
    "from src.metrics import VLAD, FisherVector\n",
    "from src.evaluate import compute_and_save_ssim_matrices, compute_and_save_ssim_matrices_train_val\n",
    "from src.config import TRANSFORMER, ROOT"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ais/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:28:49.861517Z",
     "start_time": "2024-12-18T19:28:49.858720Z"
    }
   },
   "cell_type": "code",
   "source": "root = ROOT",
   "id": "6da1d41db0ca200c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:12:35.420377Z",
     "start_time": "2024-12-17T15:12:35.373251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train')\n",
    "val_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test')\n"
   ],
   "id": "252ce43dc92346d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/Bachelorarbeit/similarity_metrics_of_images/src/datasets.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(value / 255.0, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load k-means and GMM models",
   "id": "f7e427ea78be16fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:42:41.440022Z",
     "start_time": "2024-12-12T13:42:41.425274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k_means_models = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'k_means' in model\n",
    "]\n",
    "print(\"KMeans models:\", k_means_models)\n",
    "gmm_model = [\n",
    "    model for model in os.listdir(rf'{root}/models/pickle_model_files') if 'gmm' in model\n",
    "]\n",
    "print(\"GMM models:\", gmm_model)"
   ],
   "id": "f39dbc30a3e88125",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans models: ['k_means_model_k256_root_sift.pkl', 'k_means_model_k64_sift.pkl', 'k_means_model_k16_root_sift.pkl', 'k_means_model_k32_sift.pkl', 'k_means_model_k16_sift.pkl', 'k_means_model_k32_root_sift.pkl', 'k_means_model_k256_sift.pkl', 'k_means_model_k64_root_sift.pkl', 'k_means_model_k128_sift.pkl', 'k_means_model_k24_root_sift.pkl', 'k_means_model_k128_root_sift.pkl', 'k_means_model_k24_sift.pkl']\n",
      "GMM models: ['gmm_model_k32_sift.pkl', 'gmm_model_k256_sift.pkl', 'gmm_model_k256_root_sift.pkl', 'gmm_model_k64_root_sift.pkl', 'gmm_model_k16_root_sift.pkl', 'gmm_model_k24_sift.pkl', 'gmm_model_k24_root_sift.pkl', 'gmm_model_k64_sift.pkl', 'gmm_model_k32_root_sift.pkl', 'gmm_model_k16_sift.pkl', 'gmm_model_k128_root_sift.pkl', 'gmm_model_k128_sift.pkl']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save VLAD vector matrix in `HD5` format",
   "id": "52cd18776958058a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:18:58.632385Z",
     "start_time": "2024-12-12T13:47:30.467728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(rf'{root}/res/vlad/train'):\n",
    "    os.makedirs(rf'{root}/res/vlad/train')\n",
    "\n",
    "if not os.path.exists(rf'{root}/res/vlad/validation'):\n",
    "    os.makedirs(rf'{root}/res/vlad/validation')\n",
    "\n",
    "for model in k_means_models:\n",
    "    num_clusters = int(re.findall(r'\\d+', model)[0])\n",
    "    vect_length = 128 * num_clusters if not 'pca' in model else 128 * num_clusters // 2\n",
    "    print(f\"Number of clusters: {num_clusters}, Vector length: {vect_length}\")\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    feature = 'root_sift' if 'root' in model else 'sift'\n",
    "    for img, *_, path in train_dataset:\n",
    "\n",
    "        vlad = VLAD(\n",
    "            image=img,\n",
    "            k_means=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "            flatten=True,\n",
    "            feature=feature\n",
    "        ).vector\n",
    "        if len(vlad) != vect_length:\n",
    "            raise ValueError(f\"Expected {vect_length}, got {len(vlad)}\")\n",
    "        path = os.path.basename(path)\n",
    "        train_data[path] = vlad\n",
    "\n",
    "    for img, *_, path in val_dataset:\n",
    "\n",
    "        vlad = VLAD(\n",
    "            image=img,\n",
    "            k_means=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "            flatten=True,\n",
    "            feature=feature\n",
    "        ).vector\n",
    "        if len(vlad) != vect_length:\n",
    "            raise ValueError(f\"Expected {vect_length}, got {len(vlad)}\")\n",
    "        path = os.path.basename(path)\n",
    "        val_data[path] = vlad\n",
    "\n",
    "    model_name = model.replace('.pkl', '')\n",
    "    save_to_hdf5(rf'{root}/res/vlad/train/{model_name}.h5', train_data)\n",
    "    save_to_hdf5(rf'{root}/res/vlad/validation/{model_name}.h5', val_data)"
   ],
   "id": "a13dbc56dc81e044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 256, Vector length: 32768\n",
      "Number of clusters: 64, Vector length: 8192\n",
      "Number of clusters: 16, Vector length: 2048\n",
      "Number of clusters: 32, Vector length: 4096\n",
      "Number of clusters: 16, Vector length: 2048\n",
      "Number of clusters: 32, Vector length: 4096\n",
      "Number of clusters: 256, Vector length: 32768\n",
      "Number of clusters: 64, Vector length: 8192\n",
      "Number of clusters: 128, Vector length: 16384\n",
      "Number of clusters: 24, Vector length: 3072\n",
      "Number of clusters: 128, Vector length: 16384\n",
      "Number of clusters: 24, Vector length: 3072\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute and save Fisher vector matrix in `HD5` format",
   "id": "18a6d20ce475b22c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:07:38.364114Z",
     "start_time": "2024-12-12T14:23:14.242783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(rf'{root}/res/fisher/train'):\n",
    "    os.makedirs(rf'{root}/res/fisher/train')\n",
    "\n",
    "if not os.path.exists(rf'{root}/res/fisher/validation'):\n",
    "    os.makedirs(rf'{root}/res/fisher/validation')\n",
    "\n",
    "for model in gmm_model:\n",
    "    num_clusters = int(re.findall(r'\\d+', model)[0])\n",
    "    vect_length = (2 * 128 * num_clusters + num_clusters) if not 'pca' in model else (\n",
    "                                                                                                 2 * 128 * num_clusters + num_clusters) // 2\n",
    "    print(f\"Number of clusters: {num_clusters}, Vector length: {vect_length}\")\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    feature = 'root_sift' if 'root' in model else 'sift'\n",
    "\n",
    "    for img, *_, path in train_dataset:\n",
    "\n",
    "        fisher = FisherVector(\n",
    "            image=img,\n",
    "            gmm=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "            flatten=True,\n",
    "            feature=feature\n",
    "        ).vector\n",
    "        if len(fisher) != vect_length:\n",
    "            raise ValueError(f\"Expected {vect_length}, got {len(fisher)}\")\n",
    "        path = os.path.basename(path)\n",
    "        train_data[path] = fisher\n",
    "\n",
    "    for img, *_, path in val_dataset:\n",
    "\n",
    "        fisher = FisherVector(\n",
    "            image=img,\n",
    "            gmm=load_model(rf'{root}/models/pickle_model_files/{model}'),\n",
    "            flatten=True,\n",
    "            feature=feature\n",
    "        ).vector\n",
    "        if len(fisher) != vect_length:\n",
    "            raise ValueError(f\"Expected {vect_length}, got {len(fisher)}\")\n",
    "        path = os.path.basename(path)\n",
    "        val_data[path] = fisher\n",
    "\n",
    "    model_name = model.replace('.pkl', '')\n",
    "    save_to_hdf5(rf'{root}/res/fisher/train/{model_name}.h5', train_data)\n",
    "    save_to_hdf5(rf'{root}/res/fisher/validation/{model_name}.h5', val_data)"
   ],
   "id": "62461d6f5a4b6d57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 32, Vector length: 8224\n",
      "Number of clusters: 256, Vector length: 65792\n",
      "Number of clusters: 256, Vector length: 65792\n",
      "Number of clusters: 64, Vector length: 16448\n",
      "Number of clusters: 16, Vector length: 4112\n",
      "Number of clusters: 24, Vector length: 6168\n",
      "Number of clusters: 24, Vector length: 6168\n",
      "Number of clusters: 64, Vector length: 16448\n",
      "Number of clusters: 32, Vector length: 8224\n",
      "Number of clusters: 16, Vector length: 4112\n",
      "Number of clusters: 128, Vector length: 32896\n",
      "Number of clusters: 128, Vector length: 32896\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SSIM\n",
    "\n",
    "1. Choosing kernel size for `gaussian_blur` function\n",
    "\n",
    "Using the empirical rule, the kernel radius should span 3 times the standard deviation. Which means:\n",
    "\n",
    "```python\n",
    "kernel_radius = int(3 * sigma)\n",
    "kernel_size = 2 * kernel_radius + 1 # In order that the kernel is centered around the central pixel\n",
    "```"
   ],
   "id": "76af4268817956e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:04:31.838110Z",
     "start_time": "2024-12-17T16:04:31.793993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='train', transform=TRANSFORMER)\n",
    "val_dataset = ExcavatorDataset(return_type='image+mask+path', purpose='test', transform=TRANSFORMER)"
   ],
   "id": "63e8db1ddb00359a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ais/Bachelorarbeit/similarity_metrics_of_images/src/datasets.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(value / 255.0, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A) Compute SSIM Matrix within the dataset\n",
    "\n",
    "In the code below, the data is first saved as:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'image_paths': List[str],\n",
    "    'ssim': np.ndarray,\n",
    "    'ms_ssim': np.ndarray\n",
    "}\n",
    "```\n",
    "because of computational constraints (could takr up to 16 hours/iteration)."
   ],
   "id": "b959ba99d40bb635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T20:07:04.434785Z",
     "start_time": "2024-12-17T16:10:31.432481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = f'{root}/res/ssim/within_train'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "batch_size = 20\n",
    "gaussian_sigmas = [i for i in range(0, 12, 2)]  # [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "for sigma in gaussian_sigmas:\n",
    "    compute_and_save_ssim_matrices(dataset=train_dataset,\n",
    "                                   output_dir=output_dir,\n",
    "                                   batch_size=batch_size,\n",
    "                                   sigma=sigma)"
   ],
   "id": "a7e17318ea676a7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [33:58<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=0, kernel_size=None, compression_quality=None\n",
      "Kernel size used for sigma=2: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [34:05<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=2, kernel_size=13, compression_quality=None\n",
      "Kernel size used for sigma=4: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [34:08<00:00, 38.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=4, kernel_size=25, compression_quality=None\n",
      "Kernel size used for sigma=6: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [35:27<00:00, 37.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=6, kernel_size=37, compression_quality=None\n",
      "Kernel size used for sigma=8: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [33:53<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=8, kernel_size=49, compression_quality=None\n",
      "Kernel size used for sigma=10: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM:: 100%|██████████| 79344/79344 [33:54<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SSIM and MS-SSIM matrices using: \n",
      "sigma=10, kernel_size=61, compression_quality=None\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:03:55.424507Z",
     "start_time": "2024-12-17T15:54:15.947285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: verify that ssim data is computed correctly\n",
    "# ssim_data = load_hdf5(f'{root}/res/ssim/within_train/ssim_sigma0.h5')\n",
    "# for i, key in enumerate(ssim_data.keys()):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(key, ssim_data[key].shape)\n",
    "#     print(\"SSIM has shape:\", ssim_data[key]['ssim'].shape)\n",
    "#     print(\"MS-SSIM has shape:\", ssim_data[key]['ms_ssim'].shape)\n",
    "#     print(ssim_data[key])\n"
   ],
   "id": "9bc8c1f1c974990b",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpth\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m@@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpth_2\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m         reformatted[key] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mssim\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(ssim_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mssim\u001B[39m\u001B[38;5;124m'\u001B[39m][i][j]),\n\u001B[1;32m     10\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mms_ssim\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(ssim_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mms_ssim\u001B[39m\u001B[38;5;124m'\u001B[39m][i][j])}\n\u001B[0;32m---> 11\u001B[0m \u001B[43msave_to_hdf5\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mroot\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/res/ssim/within_train/ssim_sigma\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msigma\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreformatted\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:321\u001B[0m, in \u001B[0;36msave_to_hdf5\u001B[0;34m(file_path, dataset_dict)\u001B[0m\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;28;01mcase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01m_\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported data type for dataset \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 321\u001B[0m \u001B[43msave_to_hdf5_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:317\u001B[0m, in \u001B[0;36msave_to_hdf5.<locals>.save_to_hdf5_helper\u001B[0;34m(dataset_dict, f)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;28mdict\u001B[39m():\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;66;03m# Recursively save nested dictionaries\u001B[39;00m\n\u001B[1;32m    316\u001B[0m     group \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mcreate_group(dataset_name)\n\u001B[0;32m--> 317\u001B[0m     \u001B[43msave_to_hdf5_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mcase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01m_\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported data type for dataset \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:283\u001B[0m, in \u001B[0;36msave_to_hdf5.<locals>.save_to_hdf5_helper\u001B[0;34m(dataset_dict, f)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mmatch\u001B[39;00m data:\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;28mint\u001B[39m() \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mfloat\u001B[39m():\n\u001B[0;32m--> 283\u001B[0m         \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mcase\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mTensor():\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;66;03m# Convert Torch tensor to NumPy array\u001B[39;00m\n\u001B[1;32m    287\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/h5py/_hl/group.py:183\u001B[0m, in \u001B[0;36mGroup.create_dataset\u001B[0;34m(self, name, shape, dtype, data, **kwds)\u001B[0m\n\u001B[1;32m    180\u001B[0m         parent_path, name \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    181\u001B[0m         group \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequire_group(parent_path)\n\u001B[0;32m--> 183\u001B[0m dsid \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_new_dset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m dset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mDataset(dsid)\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dset\n",
      "File \u001B[0;32m~/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/h5py/_hl/dataset.py:165\u001B[0m, in \u001B[0;36mmake_new_dset\u001B[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    163\u001B[0m     sid \u001B[38;5;241m=\u001B[39m h5s\u001B[38;5;241m.\u001B[39mcreate_simple(shape, maxshape)\n\u001B[0;32m--> 165\u001B[0m dset_id \u001B[38;5;241m=\u001B[39m \u001B[43mh5d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdcpl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdcpl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdapl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdapl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, Empty)):\n\u001B[1;32m    168\u001B[0m     dset_id\u001B[38;5;241m.\u001B[39mwrite(h5s\u001B[38;5;241m.\u001B[39mALL, h5s\u001B[38;5;241m.\u001B[39mALL, data)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T20:52:53.190928Z",
     "start_time": "2024-12-18T19:42:38.296527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ssim_data = load_hdf5(f'{root}/res/ssim/train_vs_val/ssim_train_val_sigma0.h5')\n",
    "ssim_data.keys()"
   ],
   "id": "819803f18da51ac1",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ssim_data \u001B[38;5;241m=\u001B[39m \u001B[43mload_hdf5\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mroot\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/res/ssim/train_vs_val/ssim_train_val_sigma0.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m ssim_data\u001B[38;5;241m.\u001B[39mkeys()\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:350\u001B[0m, in \u001B[0;36mload_hdf5\u001B[0;34m(file_path)\u001B[0m\n\u001B[1;32m    347\u001B[0m         data[attr_key] \u001B[38;5;241m=\u001B[39m attr_val\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_hdf5_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Bachelorarbeit/similarity_metrics_of_images/src/utils.py:333\u001B[0m, in \u001B[0;36mload_hdf5.<locals>.load_hdf5_helper\u001B[0;34m(file)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_hdf5_helper\u001B[39m(file: h5py\u001B[38;5;241m.\u001B[39mFile) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28many\u001B[39m]:\n\u001B[1;32m    332\u001B[0m     data \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 333\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh5py\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGroup\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# Recursively load groups as nested dictionaries\u001B[39;49;00m\n\u001B[1;32m    336\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mload_hdf5_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/h5py/_hl/base.py:434\u001B[0m, in \u001B[0;36mItemsViewHDF5.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m phil:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mapping:\n\u001B[0;32m--> 434\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m (key, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mapping\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/h5py/_hl/group.py:400\u001B[0m, in \u001B[0;36mGroup.get\u001B[0;34m(self, name, default, getclass, getlink)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (getclass \u001B[38;5;129;01mor\u001B[39;00m getlink):\n\u001B[1;32m    399\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 400\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m default\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/.virtualenvs/similarity_metrics_of_images/lib/python3.12/site-packages/h5py/_hl/group.py:357\u001B[0m, in \u001B[0;36mGroup.__getitem__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    355\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid HDF5 object reference\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, (\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m)):\n\u001B[0;32m--> 357\u001B[0m     oid \u001B[38;5;241m=\u001B[39m \u001B[43mh5o\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_e\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlapl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lapl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccessing a group is done with bytes or str, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    360\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(name)))\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/h5o.pyx:257\u001B[0m, in \u001B[0;36mh5py.h5o.open\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/h5i.pyx:47\u001B[0m, in \u001B[0;36mh5py.h5i.wrap_identifier\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:645\u001B[0m, in \u001B[0;36mparent\u001B[0;34m(self)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## B) Compute SSIM Matrix between train and validation datasets",
   "id": "dc8c101bbac356a5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-17T20:07:04.976886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = f'{root}/res/ssim/train_vs_val'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "# TODO: compute for sigma = 10 (still missing)\n",
    "\n",
    "batch_size = 20\n",
    "gaussian_sigmas = [i for i in range(0, 12, 2)]  # [0, 2, 4, 6, 8, 10]\n",
    "for sigma in gaussian_sigmas:\n",
    "    compute_and_save_ssim_matrices_train_val(train_dataset=train_dataset,\n",
    "                                             val_dataset=val_dataset,\n",
    "                                             output_dir=output_dir,\n",
    "                                             batch_size=batch_size,\n",
    "                                             sigma=sigma)\n",
    "\n"
   ],
   "id": "131baef845dc4cb7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SSIM/MS-SSIM (val vs train):  29%|██▉       | 54/187 [38:02<4:08:51, 112.27s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44ac63abf38da598"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
